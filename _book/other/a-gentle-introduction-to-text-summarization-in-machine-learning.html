
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>浅谈机器学习中的文本摘要 · 每周翻译计划</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        <meta name="author" content="JalanJiang.江子抑 <jjy@meitu.com>">
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-disqus/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-prism/prism-twilight.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-tbfed-pagefooter/footer.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-page-toc-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    
    <link rel="prev" href="the-downsides-of-json-for-config-files.html" />
    

    
        <link rel="shortcut icon" href='../favicon.ico' type="image/x-icon">
    
    
        <link rel="bookmark" href='../favicon.ico' type="image/x-icon">
    
    
    

    <style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
    <script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"JalanJiang","repo":"weekly-translation","type":"star","size":"small"}]};
    </script>

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    
    
        
        <li>
            <a href="http://jalan.space" target="_blank" class="custom-link">我的博客</a>
        </li>
    
    

    
    <li class="divider"></li>
    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                        <b>1.1.</b>
                    
                    简介
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">架构</li>
        
        
    
        <li class="chapter " data-level="2.1" data-path="../architecture/how-are-serverless-computing-and-platform-as-a-service-different.html">
            
                <a href="../architecture/how-are-serverless-computing-and-platform-as-a-service-different.html">
            
                    
                        <b>2.1.</b>
                    
                    Serverless 和 PaaS 的区别
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">编程语言</li>
        
        
    
        <li class="chapter " data-level="3.1" data-path="../language/types-are-moving-to-the-right.html">
            
                <a href="../language/types-are-moving-to-the-right.html">
            
                    
                        <b>3.1.</b>
                    
                    类型声明右移的原因
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.2" data-path="../language/simple-jwt-implementation-in-nodejs.html">
            
                <a href="../language/simple-jwt-implementation-in-nodejs.html">
            
                    
                        <b>3.2.</b>
                    
                    Node.js 的简单 JWT 实现
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">其他</li>
        
        
    
        <li class="chapter " data-level="4.1" data-path="follow-these-logging-best-practices-to-get-the-most-out-of-application-level-logging-slides.html">
            
                <a href="follow-these-logging-best-practices-to-get-the-most-out-of-application-level-logging-slides.html">
            
                    
                        <b>4.1.</b>
                    
                    日志记录的最佳实践
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.2" data-path="the-downsides-of-json-for-config-files.html">
            
                <a href="the-downsides-of-json-for-config-files.html">
            
                    
                        <b>4.2.</b>
                    
                    用 JSON 作为配置文件的缺陷
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="4.3" data-path="a-gentle-introduction-to-text-summarization-in-machine-learning.html">
            
                <a href="a-gentle-introduction-to-text-summarization-in-machine-learning.html">
            
                    
                        <b>4.3.</b>
                    
                    浅谈机器学习中的文本摘要
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >浅谈机器学习中的文本摘要</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <ul>
<li>&#x539F;&#x6587;&#xFF1A;<a href="https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/" target="_blank">A Gentle Introduction to Text Summarization in Machine Learning</a></li>
<li>&#x4F5C;&#x8005;&#xFF1A;<a href="https://blog.floydhub.com/author/alfrick/" target="_blank">Alfrick Opidi</a></li>
<li>&#x7FFB;&#x8BD1;&#xFF1A;<a href="http://gagalee.ink" target="_blank">Gaga Lee</a></li>
</ul>
<h1 id="&#x6D45;&#x8C08;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x4E2D;&#x7684;&#x6587;&#x672C;&#x6458;&#x8981;">&#x6D45;&#x8C08;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x4E2D;&#x7684;&#x6587;&#x672C;&#x6458;&#x8981;</h1>
<blockquote>
<p>Have you ever summarized a lengthy document into a short paragraph? How long did you take? Manually generating a summary can be time consuming and tedious. Automatic text summarization promises to overcome such difficulties and allow you to generate the key ideas in a piece of writing easily.</p>
</blockquote>
<p>&#x4F60;&#x6709;&#x6CA1;&#x6709;&#x5C1D;&#x8BD5;&#x8FC7;&#x5C06;&#x4E00;&#x6BB5;&#x5197;&#x957F;&#x7684;&#x6587;&#x672C;&#x603B;&#x7ED3;&#x4E3A;&#x4E00;&#x6BB5;&#x7B80;&#x5355;&#x7684;&#x6587;&#x672C;&#xFF1F;&#x4F60;&#x9700;&#x8981;&#x82B1;&#x591A;&#x957F;&#x65F6;&#x95F4;&#x5462;&#xFF1F;&#x624B;&#x52A8;&#x6765;&#x751F;&#x6210;&#x603B;&#x7ED3;&#x662F;&#x4E00;&#x4E2A;&#x65E2;&#x6D6A;&#x8D39;&#x65F6;&#x95F4;&#x53C8;&#x8D85;&#x6CA1;&#x6709;&#x610F;&#x601D;&#x7684;&#x4E8B;&#x60C5;&#x3002;&#x81EA;&#x52A8;&#x6587;&#x672C;&#x6458;&#x8981;&#x529F;&#x80FD;&#x4E3A;&#x6211;&#x4EEC;&#x63D0;&#x4F9B;&#x4E86;&#x4E00;&#x4E2A;&#x514B;&#x670D;&#x8FD9;&#x4E2A;&#x56F0;&#x96BE;&#x7684;&#x65B9;&#x6CD5;&#xFF0C;&#x5E76;&#x4E14;&#x53EF;&#x4EE5;&#x8BA9;&#x4F60;&#x5F88;&#x65B9;&#x4FBF;&#x5730;&#x83B7;&#x5F97;&#x4E00;&#x6BB5;&#x6587;&#x672C;&#x4E2D;&#x7684;&#x4E3B;&#x8981;&#x89C2;&#x70B9;&#x3002;</p>
<blockquote>
<p>Text summarization is the technique for generating a concise and precise summary of voluminous texts while focusing on the sections that convey useful information, and without losing the overall meaning.</p>
</blockquote>
<p>&#x5F53;&#x5728;&#x6211;&#x4EEC;&#x5173;&#x6CE8;&#x4E00;&#x4E9B;&#x4F20;&#x9012;&#x6709;&#x6548;&#x4FE1;&#x606F;&#x7684;&#x7AE0;&#x8282;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x6587;&#x672C;&#x6458;&#x8981;&#x6280;&#x672F;&#x53EF;&#x4EE5;&#x5E2E;&#x52A9;&#x6211;&#x4EEC;&#x63D0;&#x70BC;&#x7B80;&#x77ED;&#x4E14;&#x7CBE;&#x786E;&#x7684;&#x6458;&#x8981;&#xFF0C;&#x800C;&#x4E14;&#x4E0D;&#x5931;&#x53BB;&#x6574;&#x4F53;&#x542B;&#x4E49;&#x3002;</p>
<blockquote>
<p>Automatic text summarization aims to transform lengthy documents into shortened versions, something which could be difficult and costly to undertake if done manually. </p>
</blockquote>
<p>&#x81EA;&#x52A8;&#x6587;&#x672C;&#x6458;&#x8981;&#x7684;&#x76EE;&#x6807;&#x662F;&#x5C06;&#x5197;&#x957F;&#x7684;&#x6587;&#x672C;&#x8F6C;&#x6362;&#x6210;&#x66F4;&#x77ED;&#x7684;&#x7248;&#x672C;&#xFF0C;&#x800C;&#x8FD9;&#x79CD;&#x8F6C;&#x6362;&#x5982;&#x679C;&#x624B;&#x52A8;&#x5236;&#x4F5C;&#x7684;&#x8BDD;&#xFF0C;&#x6709;&#x4E00;&#x5B9A;&#x96BE;&#x5EA6;&#x4E14;&#x9700;&#x8981;&#x82B1;&#x8D39;&#x5927;&#x4EE3;&#x4EF7;&#x3002;</p>
<blockquote>
<p>Machine learning algorithms can be trained to comprehend documents and identify the sections that convey important facts and information before producing the required summarized texts. For example, the image below is of <a href="https://www.autosport.com/motogp/news/142779/marquez-calls-austin-crash-hard-to-understand" target="_blank">this news article</a> that has been fed into a machine learning algorithm to generate a summary.</p>
</blockquote>
<p>&#x673A;&#x5668;&#x5B66;&#x4E60;&#x7B97;&#x6CD5;&#x5728;&#x5236;&#x4F5C;&#x6240;&#x9700;&#x8981;&#x7684;&#x6587;&#x672C;&#x6458;&#x8981;&#x4E4B;&#x524D;&#xFF0C;&#x8981;&#x8BAD;&#x7EC3;&#x5982;&#x4F55;&#x53BB;&#x7406;&#x89E3;&#x6587;&#x672C;&#x7684;&#x7EC4;&#x6210;&#xFF0C;&#x5E76;&#x8BC6;&#x522B;&#x90A3;&#x4E9B;&#x4F20;&#x9012;&#x91CD;&#x8981;&#x4E8B;&#x5B9E;&#x548C;&#x4FE1;&#x606F;&#x7684;&#x6BB5;&#x843D;&#x3002;&#x4E3E;&#x4E2A;&#x6817;&#x5B50;&#xFF0C;&#x4E0B;&#x56FE;&#x4E2D;&#x8FD9;&#x7BC7;&#x65B0;&#x95FB;&#x6587;&#x7AE0;&#x5C31;&#x88AB;&#x9001;&#x5165;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x7B97;&#x6CD5;&#x53BB;&#x751F;&#x6210;&#x6458;&#x8981;&#x3002;</p>
<h2 id="&#x6587;&#x672C;&#x6458;&#x8981;&#x7684;&#x9700;&#x6C42;">&#x6587;&#x672C;&#x6458;&#x8981;&#x7684;&#x9700;&#x6C42;</h2>
<blockquote>
<p>With the present explosion of data circulating the digital space, which is mostly non-structured textual data, there is a need to develop automatic text summarization tools that allow people to get insights from them easily. Currently, we enjoy quick access to enormous amounts of information. However, most of this information is redundant, insignificant, and may not convey the intended meaning. For example, if you are looking for specific information from an online news article, you may have to dig through its content and spend a lot of time weeding out the unnecessary stuff before getting the information you want. Therefore, using automatic text summarizers capable of extracting useful information that leaves out inessential and insignificant data is becoming vital. Implementing summarization can enhance the readability of documents, reduce the time spent in researching for information, and allow for more information to be fitted in a particular area.</p>
</blockquote>
<p>&#x968F;&#x7740;&#x73B0;&#x5728;&#x6570;&#x636E;&#x5B58;&#x50A8;&#x7A7A;&#x95F4;&#x4E2D;&#x7684;&#x6D41;&#x901A;&#x6570;&#x636E;&#x7684;&#x7206;&#x70B8;&#x5F0F;&#x589E;&#x957F;&#xFF0C;&#x4E14;&#x5927;&#x90E8;&#x5206;&#x662F;&#x6CA1;&#x6709;&#x7ED3;&#x6784;&#x5316;&#x7684;&#x6587;&#x672C;&#x6570;&#x636E;&#xFF0C;&#x6240;&#x4EE5;&#x5F00;&#x53D1;&#x51FA;&#x4E00;&#x6B3E;&#x53EF;&#x4EE5;&#x5E2E;&#x52A9;&#x4EBA;&#x4EEC;&#x66F4;&#x65B9;&#x4FBF;&#x7684;&#x4ECE;&#x6587;&#x672C;&#x4E2D;&#x83B7;&#x53D6;&#x4E3B;&#x8981;&#x89C2;&#x70B9;&#x7684;&#x6587;&#x672C;&#x6458;&#x8981;&#x5DE5;&#x5177;&#x662F;&#x5341;&#x5206;&#x5FC5;&#x8981;&#x7684;&#x3002;&#x76EE;&#x524D;&#xFF0C;&#x6211;&#x4EEC;&#x4EAB;&#x53D7;&#x7740;&#x5FEB;&#x901F;&#x83B7;&#x53D6;&#x5230;&#x6D77;&#x91CF;&#x4FE1;&#x606F;&#x7684;&#x4FBF;&#x6377;&#x3002;&#x7136;&#x800C;&#xFF0C;&#x8FD9;&#x4E9B;&#x4FE1;&#x606F;&#x4E2D;&#x5927;&#x90E8;&#x5206;&#x662F;&#x591A;&#x4F59;&#x7684;&#xFF0C;&#x65E0;&#x5173;&#x7D27;&#x8981;&#x7684;&#xFF0C;&#x800C;&#x4E14;&#x4E0D;&#x80FD;&#x4F20;&#x9012;&#x539F;&#x4E49;&#x3002;&#x4E3E;&#x4E2A;&#x6817;&#x5B50;&#xFF0C;&#x5047;&#x8BBE;&#x4F60;&#x6B63;&#x5728;&#x4ECE;&#x4E00;&#x7BC7;&#x5728;&#x7EBF;&#x7684;&#x65B0;&#x95FB;&#x4E2D;&#x5BFB;&#x627E;&#x7279;&#x5B9A;&#x7684;&#x4FE1;&#x606F;&#xFF0C;&#x4F60;&#x5728;&#x83B7;&#x5F97;&#x4F60;&#x60F3;&#x8981;&#x7684;&#x4FE1;&#x606F;&#x4E4B;&#x524D;&#xFF0C;&#x5FC5;&#x987B;&#x5F97;&#x4ECE;&#x6587;&#x7AE0;&#x5185;&#x5BB9;&#x4E2D;&#x6316;&#x6398;&#x4FE1;&#x606F;&#xFF0C;&#x5E76;&#x82B1;&#x597D;&#x957F;&#x7684;&#x65F6;&#x95F4;&#x53BB;&#x5254;&#x9664;&#x4E0D;&#x5FC5;&#x8981;&#x7684;&#x4E1C;&#x897F;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x80FD;&#x591F;&#x5229;&#x7528;&#x81EA;&#x52A8;&#x6587;&#x672C;&#x6458;&#x8981;&#x6765;&#x5254;&#x9664;&#x65E0;&#x6548;&#x6216;&#x8005;&#x4E0D;&#x91CD;&#x8981;&#x7684;&#x4FE1;&#x606F;&#xFF0C;&#x7136;&#x540E;&#x63D0;&#x53D6;&#x6709;&#x6548;&#x4FE1;&#x606F;&#xFF0C;&#x8FD9;&#x6B63;&#x53D8;&#x5F97;&#x8D8A;&#x6765;&#x8D8A;&#x91CD;&#x8981;&#x3002;&#x5B9E;&#x73B0;&#x4E86;&#x6458;&#x8981;&#x53EF;&#x4EE5;&#x6709;&#x52A9;&#x4E8E;&#x63D0;&#x5347;&#x6587;&#x672C;&#x7684;&#x53EF;&#x8BFB;&#x6027;&#xFF0C;&#x51CF;&#x5C11;&#x4E86;&#x5BFB;&#x627E;&#x4FE1;&#x606F;&#x7684;&#x65F6;&#x95F4;&#xFF0C;&#x5E76;&#x5141;&#x8BB8;&#x7279;&#x5B9A;&#x7684;&#x8303;&#x56F4;&#x5185;&#x53EF;&#x4EE5;&#x5305;&#x542B;&#x66F4;&#x591A;&#x7684;&#x4FE1;&#x606F;&#x3002;</p>
<h2 id="&#x6587;&#x672C;&#x6458;&#x8981;&#x7684;&#x4E3B;&#x8981;&#x7C7B;&#x578B;">&#x6587;&#x672C;&#x6458;&#x8981;&#x7684;&#x4E3B;&#x8981;&#x7C7B;&#x578B;</h2>
<blockquote>
<p>Broadly, there are two approaches to summarizing texts in NLP: extraction and abstraction.</p>
</blockquote>
<p>&#x4E00;&#x822C;&#x6765;&#x8BF4;&#xFF0C;&#x5E94;&#x7528; NLP &#x6765;&#x8FDB;&#x884C;&#x6587;&#x672C;&#x6458;&#x8981;&#x7684;&#x65B9;&#x6CD5;&#x6709;&#x4E24;&#x79CD;&#xFF1A;&#x63D0;&#x53D6;&#x548C;&#x62BD;&#x8C61;&#x3002;</p>
<h3 id="&#x57FA;&#x4E8E;&#x63D0;&#x53D6;&#x7684;&#x6458;&#x8981;">&#x57FA;&#x4E8E;&#x63D0;&#x53D6;&#x7684;&#x6458;&#x8981;</h3>
<blockquote>
<p>In extraction-based summarization, a subset of words that represent the most important points is pulled from a piece of text and combined to make a summary. Think of it as a highlighter&#x2014;which selects the main information from a source text.</p>
</blockquote>
<p>&#x5728;&#x57FA;&#x4E8E;&#x63D0;&#x53D6;&#x7684;&#x6458;&#x8981;&#x4E2D;&#xFF0C;&#x5C06;&#x4ECE;&#x6587;&#x672C;&#x4E2D;&#x63D0;&#x53D6;&#x51FA;&#x53EF;&#x4EE5;&#x6307;&#x51FA;&#x91CD;&#x8981;&#x89C2;&#x70B9;&#x7684;&#x8BCD;&#x6C47;&#x81EA;&#x5DF1;&#xFF0C;&#x5E76;&#x7EC4;&#x5408;&#x5F62;&#x6210;&#x4E00;&#x4E2A;&#x6458;&#x8981;&#x3002;&#x53EF;&#x4EE5;&#x628A;&#x5B83;&#x770B;&#x505A;&#x4ECE;&#x6E90;&#x6587;&#x672C;&#x4E2D;&#x9009;&#x53D6;&#x4E00;&#x4E9B;&#x4E3B;&#x8981;&#x4FE1;&#x606F;&#x8FDB;&#x884C;&#x9AD8;&#x4EAE;&#x3002;</p>
<blockquote>
<p>In machine learning, extractive summarization usually involves weighing the essential sections of sentences and using the results to generate summaries.</p>
</blockquote>
<p>&#x5728;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x4E2D;&#xFF0C;&#x63D0;&#x53D6;&#x6587;&#x672C;&#x6458;&#x8981;&#x7ECF;&#x5E38;&#x6D89;&#x53CA;&#x5230;&#x8BA1;&#x7B97;&#x53E5;&#x5B50;&#x7684;&#x5FC5;&#x8981;&#x7EC4;&#x6210;&#x90E8;&#x5206;&#x7684;&#x6743;&#x91CD;&#xFF0C;&#x5E76;&#x4E14;&#x5229;&#x7528;&#x8FD9;&#x4E2A;&#x7ED3;&#x679C;&#x6765;&#x751F;&#x6210;&#x6458;&#x8981;&#x3002;</p>
<blockquote>
<p>Different types of algorithms and methods can be used to gauge the weights of the sentences and then rank them according to their relevance and similarity with one another&#x2014;and further joining them to generate a summary.</p>
</blockquote>
<p>&#x4E0D;&#x540C;&#x7C7B;&#x578B;&#x7684;&#x7B97;&#x6CD5;&#x548C;&#x65B9;&#x6CD5;&#x53EF;&#x4EE5;&#x7528;&#x6765;&#x6D4B;&#x91CF;&#x53E5;&#x5B50;&#x7684;&#x6743;&#x91CD;&#xFF0C;&#x5E76;&#x4E14;&#x6839;&#x636E;&#x4ED6;&#x4EEC;&#x7684;&#x76F8;&#x5173;&#x6027;&#x548C;&#x76F8;&#x4F3C;&#x6027;&#x5BF9;&#x4ED6;&#x4EEC;&#x8FDB;&#x884C;&#x6392;&#x5E8F;&#xFF0C;&#x5E76;&#x8FDB;&#x4E00;&#x6B65;&#x7684;&#x5C06;&#x4ED6;&#x4EEC;&#x8FDE;&#x63A5;&#x8D77;&#x6765;&#x751F;&#x6210;&#x6458;&#x8981;&#x3002;</p>
<h3 id="&#x57FA;&#x4E8E;&#x62BD;&#x8C61;&#x7684;&#x6458;&#x8981;">&#x57FA;&#x4E8E;&#x62BD;&#x8C61;&#x7684;&#x6458;&#x8981;</h3>
<blockquote>
<p>In abstraction-based summarization, advanced deep learning techniques are applied to paraphrase and shorten the original document, just like humans do. Think of it as a pen&#x2014;which produces novel sentences that may not be part of the source document.</p>
</blockquote>
<p>&#x5728;&#x57FA;&#x4E8E;&#x62BD;&#x8C61;&#x7684;&#x6458;&#x8981;&#x4E2D;&#xFF0C;&#x524D;&#x7F6E;&#x7684;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x6280;&#x672F;&#x88AB;&#x7528;&#x6765;&#x89E3;&#x91CA;&#x548C;&#x7CBE;&#x7B80;&#x539F;&#x59CB;&#x7684;&#x6587;&#x6863;&#xFF0C;&#x6B63;&#x5982;&#x4EBA;&#x7C7B;&#x7684;&#x505A;&#x6CD5;&#x3002;&#x628A;&#x5B83;&#x60F3;&#x8C61;&#x6210;&#x4E00;&#x652F;&#x94A2;&#x7B14;&#xFF0C;&#x53EF;&#x4EE5;&#x5199;&#x51FA;&#x65B0;&#x7684;&#x4E0D;&#x5C5E;&#x4E8E;&#x6E90;&#x6587;&#x6863;&#x7684;&#x4EFB;&#x4F55;&#x90E8;&#x5206;&#x7684;&#x53E5;&#x5B50;&#x3002;</p>
<blockquote>
<p>Although abstraction performs better at text summarization, developing its algorithms requires complicated deep learning techniques and sophisticated language modeling.</p>
</blockquote>
<p>&#x867D;&#x7136;&#x5728;&#x6587;&#x672C;&#x6458;&#x8981;&#x4E0A;&#xFF0C;&#x62BD;&#x8C61;&#x65B9;&#x5F0F;&#x8868;&#x73B0;&#x7684;&#x66F4;&#x597D;&#xFF0C;&#x4F46;&#x662F;&#x4E0D;&#x65AD;&#x53D1;&#x5C55;&#x5B83;&#x7684;&#x7B97;&#x6CD5;&#x8FD8;&#x9700;&#x8981;&#x590D;&#x6742;&#x7684;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x6280;&#x672F;&#x548C;&#x590D;&#x6742;&#x7684;&#x8BED;&#x8A00;&#x6A21;&#x578B;&#x3002;</p>
<blockquote>
<p>To generate plausible outputs, abstraction-based summarization approaches must address a wide variety of NLP problems, such as natural language generation, semantic representation, and inference permutation.</p>
</blockquote>
<p>&#x4E3A;&#x4E86;&#x751F;&#x6210;&#x53EF;&#x4FE1;&#x7684;&#x8F93;&#x51FA;&#x7ED3;&#x679C;&#xFF0C;&#x57FA;&#x4E8E;&#x62BD;&#x8C61;&#x7684;&#x6587;&#x672C;&#x6458;&#x8981;&#x65B9;&#x6CD5;&#x5FC5;&#x987B;&#x5904;&#x7406;&#x4E00;&#x5927;&#x5806; NLP &#x7684;&#x95EE;&#x9898;&#xFF0C;&#x6BD4;&#x5982;&#x8BF4;&#x81EA;&#x7136;&#x8BED;&#x8A00;&#x751F;&#x6210;&#x3001;&#x8BED;&#x4E49;&#x8868;&#x793A;&#x548C;&#x63A8;&#x7406;&#x6392;&#x5217;&#x3002;</p>
<blockquote>
<p>As such, extractive text summarization approaches are still widely popular. In this article, we&#x2019;ll be focusing on an extraction-based method.</p>
</blockquote>
<p>&#x56E0;&#x4E3A;&#x8FD9;&#x6837;&#xFF0C;&#x57FA;&#x4E8E;&#x62BD;&#x53D6;&#x7684;&#x6587;&#x672C;&#x6458;&#x8981;&#x65B9;&#x6CD5;&#x59CB;&#x7EC8;&#x5E7F;&#x53D7;&#x6B22;&#x8FCE;&#x3002;&#x5728;&#x8FD9;&#x7BC7;&#x6587;&#x7AE0;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x4E5F;&#x5C06;&#x4E3B;&#x8981;&#x805A;&#x7126;&#x4E8E;&#x4E00;&#x4E2A;&#x57FA;&#x4E8E;&#x62BD;&#x53D6;&#x7684;&#x6587;&#x672C;&#x6458;&#x8981;&#x65B9;&#x6CD5;&#x3002;</p>
<h2 id="&#x5982;&#x4F55;&#x6267;&#x884C;&#x6587;&#x672C;&#x6458;&#x8981;">&#x5982;&#x4F55;&#x6267;&#x884C;&#x6587;&#x672C;&#x6458;&#x8981;</h2>
<blockquote>
<p>Let&#x2019;s use a short paragraph to illustrate how extractive text summarization can be performed.</p>
</blockquote>
<p>&#x8BA9;&#x6211;&#x4EEC;&#x4F7F;&#x7528;&#x4E00;&#x4E2A;&#x7B80;&#x77ED;&#x7684;&#x7247;&#x6BB5;&#x6765;&#x8BF4;&#x660E;&#xFF0C;&#x63D0;&#x53D6;&#x6587;&#x672C;&#x6458;&#x8981;&#x662F;&#x600E;&#x4E48;&#x6837;&#x6267;&#x884C;&#x7684;&#x3002;</p>
<blockquote>
<p>Here is the paragraph:</p>
</blockquote>
<p>&#x8FD9;&#x91CC;&#x6709;&#x4E2A;&#x7247;&#x6BB5;&#xFF1A;</p>
<blockquote>
<p>Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital. Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well. Therefore, Peter stayed with her at the hospital for 3 days without leaving.&#x201D;</p>
</blockquote>
<p>&#x76AE;&#x7279;&#x548C;&#x4F0A;&#x4E3D;&#x838E;&#x767D;&#x4E58;&#x5750;&#x51FA;&#x79DF;&#x8F66;&#x53BB;&#x53C2;&#x52A0;&#x57CE;&#x5E02;&#x91CC;&#x7684;&#x591C;&#x95F4;&#x6D3E;&#x5BF9;&#x3002;&#x5728;&#x6D3E;&#x5BF9;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x4F0A;&#x4E3D;&#x838E;&#x767D;&#x6655;&#x5012;&#x4E86;&#x5E76;&#x9A6C;&#x4E0A;&#x88AB;&#x9001;&#x53BB;&#x4E86;&#x533B;&#x9662;&#x3002;&#x5F53;&#x5979;&#x88AB;&#x8BCA;&#x65AD;&#x4E3A;&#x8111;&#x90E8;&#x53D7;&#x4F24;&#x4E4B;&#x540E;&#xFF0C;&#x533B;&#x751F;&#x53EE;&#x5631;&#x76AE;&#x7279;&#x8981;&#x966A;&#x62A4;&#x7740;&#x5979;&#x76F4;&#x5230;&#x5979;&#x597D;&#x8D77;&#x6765;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x76AE;&#x7279;&#x6CA1;&#x6709;&#x79BB;&#x5F00;&#x533B;&#x9662;&#xFF0C;&#x6574;&#x6574;&#x966A;&#x62A4;&#x4E86;&#x5979;&#x4E09;&#x5929;&#x3002;</p>
<blockquote>
<p>Here are the steps to follow to summarize the above paragraph, while trying to maintain its intended meaning, as much as possible.</p>
</blockquote>
<p>&#x8FD9;&#x662F;&#x603B;&#x7ED3;&#x4E0A;&#x65B9;&#x6587;&#x7AE0;&#x7247;&#x6BB5;&#x7684;&#x6B65;&#x9AA4;&#xFF0C;&#x6211;&#x4EEC;&#x5C3D;&#x53EF;&#x80FD;&#x7684;&#x8981;&#x4FDD;&#x6301;&#x5B83;&#x539F;&#x6709;&#x7684;&#x8BED;&#x4E49;&#x3002;</p>
<h3 id="&#x7B2C;&#x4E00;&#x6B65;&#xFF1A;&#x5C06;&#x6587;&#x7AE0;&#x6BB5;&#x843D;&#x8F6C;&#x6362;&#x4E3A;&#x53E5;&#x5B50;">&#x7B2C;&#x4E00;&#x6B65;&#xFF1A;&#x5C06;&#x6587;&#x7AE0;&#x6BB5;&#x843D;&#x8F6C;&#x6362;&#x4E3A;&#x53E5;&#x5B50;</h3>
<blockquote>
<p>First, let&#x2019;s split the paragraph into its corresponding sentences. The best way of doing the conversion is to extract a sentence whenever a period appears.</p>
</blockquote>
<p>&#x9996;&#x5148;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x6587;&#x7AE0;&#x6BB5;&#x843D;&#x5206;&#x5272;&#x6210;&#x5BF9;&#x5E94;&#x7684;&#x53E5;&#x5B50;&#x3002;&#x6700;&#x597D;&#x7684;&#x529E;&#x6CD5;&#x5C31;&#x662F;&#x5F53;&#x53E5;&#x53F7;&#x51FA;&#x73B0;&#x7684;&#x65F6;&#x5019;&#x8FDB;&#x884C;&#x63D0;&#x53D6;&#x3002;</p>
<blockquote>
<ol>
<li>Peter and Elizabeth took a taxi to attend the night party in the city</li>
</ol>
</blockquote>
<ol>
<li>&#x76AE;&#x7279;&#x548C;&#x4F0A;&#x4E3D;&#x838E;&#x767D;&#x4E58;&#x5750;&#x51FA;&#x79DF;&#x8F66;&#x53BB;&#x53C2;&#x52A0;&#x57CE;&#x5E02;&#x91CC;&#x7684;&#x591C;&#x95F4;&#x6D3E;&#x5BF9;</li>
</ol>
<blockquote>
<ol>
<li>While in the party, Elizabeth collapsed and was rushed to the hospital</li>
</ol>
</blockquote>
<ol>
<li>&#x5728;&#x6D3E;&#x5BF9;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x4F0A;&#x4E3D;&#x838E;&#x767D;&#x6655;&#x5012;&#x4E86;&#x5E76;&#x9A6C;&#x4E0A;&#x88AB;&#x9001;&#x53BB;&#x4E86;&#x533B;&#x9662;&#x3002;</li>
</ol>
<blockquote>
<ol>
<li>Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well</li>
</ol>
</blockquote>
<ol>
<li>&#x5F53;&#x5979;&#x88AB;&#x8BCA;&#x65AD;&#x4E3A;&#x8111;&#x90E8;&#x53D7;&#x4F24;&#x4E4B;&#x540E;&#xFF0C;&#x533B;&#x751F;&#x53EE;&#x5631;&#x76AE;&#x7279;&#x8981;&#x966A;&#x62A4;&#x7740;&#x5979;&#x76F4;&#x5230;&#x5979;&#x597D;&#x8D77;&#x6765;&#x3002;</li>
</ol>
<blockquote>
<ol>
<li>Therefore, Peter stayed with her at the hospital for 3 days without leaving</li>
</ol>
</blockquote>
<ol>
<li>&#x56E0;&#x6B64;&#xFF0C;&#x76AE;&#x7279;&#x6CA1;&#x6709;&#x79BB;&#x5F00;&#x533B;&#x9662;&#xFF0C;&#x6574;&#x6574;&#x966A;&#x62A4;&#x4E86;&#x5979;&#x4E09;&#x5929;&#x3002;</li>
</ol>
<h3 id="&#x7B2C;&#x4E8C;&#x6B65;&#xFF1A;&#x6587;&#x672C;&#x5904;&#x7406;">&#x7B2C;&#x4E8C;&#x6B65;&#xFF1A;&#x6587;&#x672C;&#x5904;&#x7406;</h3>
<blockquote>
<p>Next, let&#x2019;s do text processing by removing the stop words (extremely common words with little meaning such as &#x201C;and&#x201D; and &#x201C;the&#x201D;), numbers, punctuation, and other special characters from the sentences.</p>
</blockquote>
<p>&#x63A5;&#x7740;&#xFF0C;&#x6211;&#x4EEC;&#x901A;&#x8FC7;&#x79FB;&#x9664;&#x53E5;&#x5B50;&#x4E2D;&#x7684;&#x505C;&#x987F;&#x8BCD;&#xFF08;&#x5C24;&#x5176;&#x662F;&#x5E38;&#x89C1;&#x7684;&#x4F4E;&#x8BED;&#x4E49;&#x7684;&#x8BCD;&#x8BED;&#xFF0C;&#x6BD4;&#x5982;&#x201C;&#x548C;&#x201D;&#x4EE5;&#x53CA;&#x5B9A;&#x51A0;&#x8BCD;&#xFF09;&#xFF0C;&#x6570;&#x5B57;&#xFF0C;&#x6807;&#x70B9;&#x7B26;&#x53F7;&#x548C;&#x5176;&#x4ED6;&#x7279;&#x6B8A;&#x7684;&#x5B57;&#x7B26;&#x6765;&#x8FDB;&#x884C;&#x6587;&#x672C;&#x5904;&#x7406;&#x3002;</p>
<blockquote>
<p>Performing the filtering assists in removing redundant and insignificant information which may not provide any added value to the text&#x2019;s meaning.</p>
</blockquote>
<p>&#x6267;&#x884C;&#x8FC7;&#x6EE4;&#x5C06;&#x6709;&#x52A9;&#x4E8E;&#x79FB;&#x9664;&#x591A;&#x4F59;&#x4E14;&#x65E0;&#x7528;&#x7684;&#x4FE1;&#x606F;&#xFF0C;&#x8FD9;&#x4E9B;&#x4FE1;&#x606F;&#x5E76;&#x4E0D;&#x4F1A;&#x4E3A;&#x53E5;&#x5B50;&#x7684;&#x610F;&#x4E49;&#x589E;&#x52A0;&#x4EF7;&#x503C;&#x3002;</p>
<blockquote>
<p>Here is the result of the text processing:</p>
</blockquote>
<p>&#x4E0B;&#x9762;&#x662F;&#x6587;&#x672C;&#x5904;&#x7406;&#x540E;&#x7684;&#x7ED3;&#x679C;&#xFF1A;</p>
<blockquote>
<ol>
<li>Peter Elizabeth took taxi attend night party city</li>
</ol>
</blockquote>
<ol>
<li>&#x76AE;&#x7279; &#x4F0A;&#x4E3D;&#x838E;&#x767D; &#x4E58;&#x5750; &#x5730;&#x5740; &#x53C2;&#x52A0; &#x665A;&#x4E0A; &#x6D3E;&#x5BF9; &#x57CE;&#x5E02;</li>
</ol>
<blockquote>
<ol>
<li>Party Elizabeth collapse rush hospital</li>
</ol>
</blockquote>
<ol>
<li>&#x6D3E;&#x5BF9; &#x4F0A;&#x4E3D;&#x838E;&#x767D; &#x6655;&#x5012; &#x9A6C;&#x4E0A; &#x533B;&#x9662;</li>
</ol>
<blockquote>
<ol>
<li>Diagnose brain injury doctor told Peter stay besides get well</li>
</ol>
</blockquote>
<ol>
<li>&#x8BCA;&#x65AD; &#x5927;&#x8111; &#x53D7;&#x4F24; &#x533B;&#x751F; &#x544A;&#x77E5; &#x76AE;&#x7279; &#x7559;&#x5728; &#x8FB9;&#x4E0A; &#x8FB9;&#x597D;</li>
</ol>
<blockquote>
<ol>
<li>Peter stay hospital days without leaving</li>
</ol>
</blockquote>
<ol>
<li>&#x76AE;&#x7279; &#x7559;&#x5728; &#x533B;&#x9662; &#x5929;&#xFF08;&#x8BB8;&#x591A;&#xFF09; &#x6CA1;&#x6709;&#x79BB;&#x5F00;</li>
</ol>
<h3 id="&#x7B2C;&#x4E09;&#x6B65;&#xFF1A;&#x8BCD;&#x8BED;&#x5207;&#x5206;">&#x7B2C;&#x4E09;&#x6B65;&#xFF1A;&#x8BCD;&#x8BED;&#x5207;&#x5206;</h3>
<blockquote>
<p>Tokenizing the sentences is done to get all the words present in the sentences. Here is a list of the words:</p>
</blockquote>
<p>&#x8BCD;&#x8BED;&#x5207;&#x5206;&#x662F;&#x4E3A;&#x4E86;&#x83B7;&#x5F97;&#x53E5;&#x5B50;&#x4E2D;&#x7684;&#x6240;&#x6709;&#x8BCD;&#x8BED;&#x3002;&#x4E0B;&#x9762;&#x662F;&#x4E00;&#x4E32;&#x8BCD;&#x8BED;&#x7684;&#x5217;&#x8868;&#x3002;</p>
<pre class="language-"><code class="lang-python"><span class="token punctuation">[</span><span class="token string">&apos;peter&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;elizabeth&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;took&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;taxi&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;attend&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;night&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;party&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;city&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;party&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;elizabeth&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;collapse&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;rush&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;hospital&apos;</span><span class="token punctuation">,</span> <span class="token string">&apos;diagnose&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;brain&apos;</span><span class="token punctuation">,</span> <span class="token string">&apos;injury&apos;</span><span class="token punctuation">,</span> <span class="token string">&apos;doctor&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;told&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;peter&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;stay&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;besides&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;get&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;well&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;peter&apos;</span><span class="token punctuation">,</span> <span class="token string">&apos;stayed&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;hospital&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;days&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;without&apos;</span><span class="token punctuation">,</span><span class="token string">&apos;leaving&apos;</span><span class="token punctuation">]</span>
</code></pre>
<h3 id="&#x7B2C;&#x56DB;&#x6B65;&#xFF1A;&#x6839;&#x636E;&#x5355;&#x8BCD;&#x51FA;&#x73B0;&#x7684;&#x9891;&#x7387;&#x6765;&#x8BC4;&#x4F30;&#x6743;&#x91CD;">&#x7B2C;&#x56DB;&#x6B65;&#xFF1A;&#x6839;&#x636E;&#x5355;&#x8BCD;&#x51FA;&#x73B0;&#x7684;&#x9891;&#x7387;&#x6765;&#x8BC4;&#x4F30;&#x6743;&#x91CD;</h3>
<blockquote>
<p>Thereafter, let&#x2019;s calculate the weighted occurrence frequency of all the words. To achieve this, let&#x2019;s divide the occurrence frequency of each of the words by the frequency of the most recurrent word in the paragraph, which is &#x201C;Peter&#x201D; that occurs three times.</p>
</blockquote>
<p>&#x4E4B;&#x540E;&#xFF0C;&#x8BA9;&#x6211;&#x4EEC;&#x6765;&#x8BA1;&#x7B97;&#x4E0B;&#x6240;&#x6709;&#x51FA;&#x73B0;&#x7684;&#x5355;&#x8BCD;&#x7684;&#x52A0;&#x6743;&#x9891;&#x7387;&#x3002;&#x4E3A;&#x4E86;&#x5B8C;&#x6210;&#x8FD9;&#x4EF6;&#x4E8B;&#x513F;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x6BCF;&#x4E2A;&#x51FA;&#x73B0;&#x7684;&#x5355;&#x8BCD;&#x7684;&#x9891;&#x6570;&#x9664;&#x4EE5;&#x6BB5;&#x843D;&#x4E2D;&#x6700;&#x5E38;&#x91CD;&#x590D;&#x51FA;&#x73B0;&#x7684;&#x5355;&#x8BCD;&#xFF0C;&#x6BD4;&#x5982;&#x51FA;&#x73B0;&#x4E86;&#x4E09;&#x6B21;&#x7684;&#x201C;&#x76AE;&#x7279;&#x201D;&#x3002;</p>
<blockquote>
<p>Here is a table that gives the weighted occurrence frequency of each of the words.</p>
</blockquote>
<p>&#x4E0B;&#x9762;&#x7684;&#x8868;&#x683C;&#x4E2D;&#x5448;&#x73B0;&#x4E86;&#x6BCF;&#x4E2A;&#x5355;&#x8BCD;&#x51FA;&#x73B0;&#x7684;&#x52A0;&#x6743;&#x9891;&#x7387;&#x3002;</p>
<div style="text-align:center;">

&#x5355;&#x8BCD; | &#x9891;&#x6570; | &#x52A0;&#x6743;&#x9891;&#x7387;
---|---|---
peter | 3 | 1
elizabeth | 2 | 0.67
took | 1 | 0.33
taxi | 1 | 0.33
attend | 1 | 0.33
night | 1 | 0.33
party | 2 | 0.67
city | 1 | 0.33
collapse | 1 | 0.33
rush | 1 | 0.33
hospital | 2 | 0.67
diagnose | 1 | 0.33
brain | 1 | 0.33
injury | 1 | 0.33
doctor | 1 | 0.33
told | 1 | 0.33
stay | 2 | 0.67
besides | 1 | 0.33
get | 1 | 0.33
well | 1 | 0.33
days | 1 | 0.33
without | 1 | 0.33
leaving | 1 | 0.33

</div>

<h3 id="&#x7B2C;&#x4E94;&#x6B65;&#xFF1A;&#x6839;&#x636E;&#x5355;&#x8BCD;&#x7684;&#x52A0;&#x6743;&#x9891;&#x7387;&#x6765;&#x8FDB;&#x884C;&#x66FF;&#x4EE3;">&#x7B2C;&#x4E94;&#x6B65;&#xFF1A;&#x6839;&#x636E;&#x5355;&#x8BCD;&#x7684;&#x52A0;&#x6743;&#x9891;&#x7387;&#x6765;&#x8FDB;&#x884C;&#x66FF;&#x4EE3;</h3>
<blockquote>
<p>Let&#x2019;s substitute each of the words found in the original sentences with their weighted frequencies. Then, we&#x2019;ll compute their sum.</p>
</blockquote>
<p>&#x6211;&#x4EEC;&#x7528;&#x5355;&#x8BCD;&#x7684;&#x52A0;&#x6743;&#x9891;&#x7387;&#x6765;&#x66FF;&#x4EE3;&#x539F;&#x53E5;&#x4E2D;&#x7684;&#x5355;&#x8BCD;&#x3002;&#x7136;&#x540E;&#xFF0C;&#x6211;&#x4EEC;&#x6765;&#x8BA1;&#x7B97;&#x4E0B;&#x4ED6;&#x4EEC;&#x7684;&#x603B;&#x548C;&#x3002;</p>
<blockquote>
<p>Since the weighted frequencies of the insignificant words, such as stop words and special characters, which were removed during the processing stage, is zero, it&#x2019;s not necessary to add them.</p>
</blockquote>
<p>&#x800C;&#x5728;&#x5904;&#x7406;&#x9636;&#x6BB5;&#x88AB;&#x79FB;&#x9664;&#x7684;&#x90A3;&#x4E9B;&#x8BCD;&#x8BED;&#xFF0C;&#x6BD4;&#x5982;&#x505C;&#x987F;&#x8BCD;&#x6216;&#x8005;&#x7279;&#x6B8A;&#x7684;&#x5B57;&#x7B26;&#xFF0C;&#x4ED6;&#x4EEC;&#x7684;&#x52A0;&#x6743;&#x9891;&#x7387;&#x4E3A;&#x96F6;&#xFF0C;&#x6240;&#x4EE5;&#x5E76;&#x4E0D;&#x9700;&#x8981;&#x53BB;&#x6DFB;&#x52A0;&#x4ED6;&#x4EEC;&#x3002;</p>
<div style="text-align:center;">

&#x53E5;&#x5B50; | &#x52A0;&#x6743;&#x9891;&#x7387; | &#x52A0;&#x603B;&#x8FC7;&#x7A0B; | &#x603B;&#x548C;
---|---|---|---
1 | &#x76AE;&#x7279;&#x548C;&#x4F0A;&#x4E3D;&#x838E;&#x767D;&#x4E58;&#x5750;&#x51FA;&#x79DF;&#x8F66;&#x53BB;&#x53C2;&#x52A0;&#x57CE;&#x5E02;&#x91CC;&#x7684;&#x591C;&#x95F4;&#x6D3E;&#x5BF9; | 1 + 0.67 + 0.33 + 0.33 + 0.33 + 0.33 + 0.67 + 0.33 | 3.99
2 | &#x5728;&#x6D3E;&#x5BF9;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x4F0A;&#x4E3D;&#x838E;&#x767D;&#x6655;&#x5012;&#x4E86;&#x5E76;&#x9A6C;&#x4E0A;&#x88AB;&#x9001;&#x53BB;&#x4E86;&#x533B;&#x9662; | 0.67 + 0.67 + 0.33 + 0.33 + 0.67 | 2.67
3 | &#x5F53;&#x5979;&#x88AB;&#x8BCA;&#x65AD;&#x4E3A;&#x8111;&#x90E8;&#x53D7;&#x4F24;&#x4E4B;&#x540E;&#xFF0C;&#x533B;&#x751F;&#x53EE;&#x5631;&#x76AE;&#x7279;&#x8981;&#x966A;&#x62A4;&#x7740;&#x5979;&#x76F4;&#x5230;&#x5979;&#x597D;&#x8D77;&#x6765; | 0.33 + 0.33 + 0.33 + 0.33 + 1 + 0.33 + 0.33 + 0.33 + 0.33 +0.33 | 3.97
4 | &#x56E0;&#x6B64;&#xFF0C;&#x76AE;&#x7279;&#x6CA1;&#x6709;&#x79BB;&#x5F00;&#x533B;&#x9662;&#xFF0C;&#x6574;&#x6574;&#x966A;&#x62A4;&#x4E86;&#x5979;&#x4E09;&#x5929; | 1 + 0.67 + 0.67 + 0.33 + 0.33 + 0.33 | 3.33

</div>

<blockquote>
<p>From the sum of the weighted frequencies of the words, we can deduce that the first sentence carries the most weight in the paragraph. Therefore, it can give the best representative summary of what the paragraph is about.</p>
</blockquote>
<p>&#x4ECE;&#x5355;&#x8BCD;&#x7684;&#x6743;&#x91CD;&#x9891;&#x7387;&#x603B;&#x548C;&#x6765;&#x770B;&#xFF0C;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x63A8;&#x65AD;&#x9996;&#x53E5;&#x5728;&#x6587;&#x7AE0;&#x6BB5;&#x843D;&#x4E2D;&#x83B7;&#x5F97;&#x4E86;&#x6700;&#x9AD8;&#x6743;&#x91CD;&#x3002;&#x56E0;&#x6B64;&#xFF0C;&#x9996;&#x53E5;&#x662F;&#x53EF;&#x4EE5;&#x63D0;&#x4F9B;&#x6574;&#x4E2A;&#x6BB5;&#x843D;&#x542B;&#x4E49;&#x7684;&#x6700;&#x4F18;&#x603B;&#x7ED3;&#x3002;</p>
<blockquote>
<p>Furthermore, if the first sentence is combined with the third sentence, which is the second-most weighty sentence in the paragraph, a better summary can be generated.</p>
</blockquote>
<p>&#x6B64;&#x5916;&#xFF0C;&#x5982;&#x679C;&#x5C06;&#x9996;&#x53E5;&#x548C;&#x6BB5;&#x843D;&#x4E2D;&#x5177;&#x6709;&#x7B2C;&#x4E8C;&#x5927;&#x6743;&#x91CD;&#x7684;&#x7B2C;&#x4E09;&#x53E5;&#x7ED3;&#x5408;&#x8D77;&#x6765;&#xFF0C;&#x5C06;&#x4F1A;&#x83B7;&#x5F97;&#x4E00;&#x4E2A;&#x66F4;&#x597D;&#x7684;&#x603B;&#x7ED3;&#x3002;</p>
<blockquote>
<p>The above example just gives a basic illustration of how to perform extraction-based text summarization in machine learning. Now, let&#x2019;s see how we can apply the concept above in creating a real-world summary generator.</p>
</blockquote>
<p>&#x4E0A;&#x9762;&#x7684;&#x6817;&#x5B50;&#x53EA;&#x662F;&#x63D0;&#x4F9B;&#x4E86;&#x4E00;&#x4E2A;&#x57FA;&#x7840;&#x7684;&#x8BF4;&#x660E;&#xFF0C;&#x5982;&#x4F55;&#x5728;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x4E2D;&#x57FA;&#x4E8E;&#x63D0;&#x53D6;&#x6765;&#x83B7;&#x5F97;&#x6587;&#x672C;&#x6458;&#x8981;&#x3002;&#x73B0;&#x5728;&#xFF0C;&#x6211;&#x4EEC;&#x6765;&#x770B;&#x770B;&#x5982;&#x4F55;&#x6839;&#x636E;&#x4E0A;&#x65B9;&#x8FD9;&#x4E2A;&#x6982;&#x5FF5;&#x6765;&#x8FDB;&#x884C;&#x4E00;&#x4E2A;&#x771F;&#x5B9E;&#x53EF;&#x7528;&#x7684;&#x6458;&#x8981;&#x751F;&#x6210;&#x5668;&#x3002;</p>
<h2 id="&#x4E00;&#x7BC7;&#x7EF4;&#x57FA;&#x767E;&#x79D1;&#x6587;&#x7AE0;&#x7684;&#x6587;&#x672C;&#x6458;&#x8981;">&#x4E00;&#x7BC7;&#x7EF4;&#x57FA;&#x767E;&#x79D1;&#x6587;&#x7AE0;&#x7684;&#x6587;&#x672C;&#x6458;&#x8981;</h2>
<blockquote>
<p>Let&#x2019;s get our hands dirty by creating a text summarizer that can shorten the information found in a lengthy web article. To keep things simple, apart from Python&#x2019;s <a href="https://www.nltk.org/" target="_blank">NLTK toolkit</a>, we&#x2019;ll not use any other machine learning library.</p>
</blockquote>
<p>&#x8BA9;&#x6211;&#x4EEC;&#x6765;&#x624B;&#x52A8;&#x521B;&#x9020;&#x4E00;&#x4E2A;&#x6587;&#x672C;&#x6458;&#x8981;&#x5668;&#xFF0C;&#x53EF;&#x4EE5;&#x7528;&#x6765;&#x7CBE;&#x7B80;&#x7F51;&#x7EDC;&#x4E0A;&#x627E;&#x5230;&#x7684;&#x5197;&#x957F;&#x6587;&#x7AE0;&#x7684;&#x4FE1;&#x606F;&#x91CF;&#x3002;&#x4E3A;&#x4E86;&#x7B80;&#x5355;&#x65B9;&#x4FBF;&#x4E00;&#x70B9;&#xFF0C;&#x9664;&#x4E86; Python &#x7684; <strong>LTK toolkit</strong>&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x4E0D;&#x4F1A;&#x4F7F;&#x7528;&#x5176;&#x4ED6;&#x7684;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x51FD;&#x6570;&#x5E93;&#x3002;</p>
<blockquote>
<p>Here is the code blueprint of the summarizer:</p>
</blockquote>
<p>&#x4E0B;&#x9762;&#x662F;&#x6458;&#x8981;&#x5668;&#x7684;&#x4F2A;&#x4EE3;&#x7801;&#x6A21;&#x578B;&#xFF1A;</p>
<pre class="language-"><code class="lang-python"><span class="token comment"># Creating a dictionary for the word frequency table</span>
frequency_table <span class="token operator">=</span> _create_dictionary_table<span class="token punctuation">(</span>article<span class="token punctuation">)</span>

<span class="token comment"># Tokenizing the sentences</span>
sentences <span class="token operator">=</span> sent_tokenize<span class="token punctuation">(</span>article<span class="token punctuation">)</span>

<span class="token comment"># Algorithm for scoring a sentence by its words</span>
sentence_scores <span class="token operator">=</span> _calculate_sentence_scores<span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> frequency_table<span class="token punctuation">)</span>

<span class="token comment"># Getting the threshold</span>
threshold <span class="token operator">=</span> _calculate_average_score<span class="token punctuation">(</span>sentence_scores<span class="token punctuation">)</span>

<span class="token comment"># Producing the summary</span>
article_summary <span class="token operator">=</span> _get_article_summary<span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> sentence_scores<span class="token punctuation">,</span> <span class="token number">1.5</span> <span class="token operator">*</span> threshold<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>article_summary<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>Here are the steps for creating a simple text summarizer in Python.</p>
</blockquote>
<p>&#x4E0B;&#x9762;&#x662F;&#x7528; Python &#x6765;&#x751F;&#x6210;&#x7B80;&#x5355;&#x7684;&#x6587;&#x672C;&#x6458;&#x8981;&#x5668;&#x7684;&#x6B65;&#x9AA4;&#x3002;</p>
<h3 id="&#x7B2C;&#x4E00;&#x6B65;&#xFF1A;&#x51C6;&#x5907;&#x597D;&#x6570;&#x636E;">&#x7B2C;&#x4E00;&#x6B65;&#xFF1A;&#x51C6;&#x5907;&#x597D;&#x6570;&#x636E;</h3>
<blockquote>
<p>In this example, we want to summarize the information found on this Wikipedia article, which just gives an overview of the major happenings during the 20th century.</p>
</blockquote>
<p>&#x5728;&#x8FD9;&#x4E2A;&#x6817;&#x5B50;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x5E0C;&#x671B;&#x4ECE;&#x8FD9;&#x7BC7;&#x7EF4;&#x57FA;&#x767E;&#x79D1;&#x7684;&#x6587;&#x7AE0;&#x4E2D;&#x83B7;&#x53D6;&#x6458;&#x8981;&#x4FE1;&#x606F;&#xFF0C;&#x8FD9;&#x4E2A;&#x4FE1;&#x606F;&#x53EF;&#x4EE5;&#x56DE;&#x987E; 20 &#x4E16;&#x7EAA;&#x4E2D;&#x53D1;&#x751F;&#x7684;&#x4E3B;&#x8981;&#x7684;&#x5927;&#x4E8B;&#x4EF6;&#x3002;</p>
<blockquote>
<p>To enable us to fetch the article&#x2019;s text, we&#x2019;ll use the Beautiful Soup library.</p>
</blockquote>
<p>&#x4E3A;&#x4E86;&#x83B7;&#x53D6;&#x6587;&#x7AE0;&#x7684;&#x6587;&#x672C;&#x4FE1;&#x606F;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x4F7F;&#x7528; <strong>Beautiful Soup</strong> &#x5E93;&#x3002;</p>
<blockquote>
<p>Here is the code for scraping the article&#x2019;s content:</p>
</blockquote>
<p>&#x4E0B;&#x9762;&#x662F;&#x6293;&#x53D6;&#x6587;&#x7AE0;&#x5185;&#x5BB9;&#x7684;&#x4EE3;&#x7801;&#xFF1A;</p>
<pre class="language-"><code class="lang-python"><span class="token keyword">import</span> bs4 <span class="token keyword">as</span> BeautifulSoup
<span class="token keyword">import</span> urllib<span class="token punctuation">.</span>request  

<span class="token comment"># Fetching the content from the URL</span>
fetched_data <span class="token operator">=</span> urllib<span class="token punctuation">.</span>request<span class="token punctuation">.</span>urlopen<span class="token punctuation">(</span><span class="token string">&apos;https://en.wikipedia.org/wiki/20th_century&apos;</span><span class="token punctuation">)</span>

article_read <span class="token operator">=</span> fetched_data<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># Parsing the URL content and storing in a variable</span>
article_parsed <span class="token operator">=</span> BeautifulSoup<span class="token punctuation">.</span>BeautifulSoup<span class="token punctuation">(</span>article_read<span class="token punctuation">,</span><span class="token string">&apos;html.parser&apos;</span><span class="token punctuation">)</span>

<span class="token comment"># Returning &lt;p&gt; tags</span>
paragraphs <span class="token operator">=</span> article_parsed<span class="token punctuation">.</span>find_all<span class="token punctuation">(</span><span class="token string">&apos;p&apos;</span><span class="token punctuation">)</span>

article_content <span class="token operator">=</span> <span class="token string">&apos;&apos;</span>

<span class="token comment"># Looping through the paragraphs and adding them to the variable</span>
<span class="token keyword">for</span> p <span class="token keyword">in</span> paragraphs<span class="token punctuation">:</span>  
    article_content <span class="token operator">+=</span> p<span class="token punctuation">.</span>text
</code></pre>
<blockquote>
<p>In the above code, we begin by importing the essential libraries for fetching data from the web page. The <strong>BeautifulSoup</strong> library is used for parsing the page while the <a href="https://docs.python.org/3/library/urllib.html" target="_blank">urllib library</a> is used for connecting to the page and retrieving the HTML.</p>
</blockquote>
<p>&#x5728;&#x4E0A;&#x9762;&#x7684;&#x4EE3;&#x7801;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x4ECE;&#x5BFC;&#x5165;&#x6700;&#x5FC5;&#x8981;&#x7684;&#x7528;&#x6765;&#x6293;&#x53D6;&#x7F51;&#x9875;&#x6570;&#x636E;&#x7684;&#x5E93;&#x5F00;&#x59CB;&#x3002;<strong>BeautifulSoup</strong> &#x5E93;&#x662F;&#x7528;&#x6765;&#x89E3;&#x6790;&#x9875;&#x9762;&#x7684;&#xFF0C;&#x540C;&#x65F6;&#xFF0C;<strong>urllib library</strong> &#x5E93;&#x662F;&#x7528;&#x6765;&#x8FDE;&#x63A5;&#x7F51;&#x9875;&#x5E76;&#x68C0;&#x7D22; HTML &#x3002;</p>
<blockquote>
<p>BeautifulSoup converts the incoming text to Unicode characters and the outgoing text to UTF-8 characters, saving you the hassle of managing different charset encodings while scraping text from the web.</p>
</blockquote>
<p>BeautifulSoup &#x5C06;&#x4F20;&#x5165;&#x7684;&#x6587;&#x672C;&#x8F6C;&#x6362;&#x4E3A; Unicode &#x5B57;&#x7B26;&#xFF0C;&#x5E76;&#x5C06;&#x53D1;&#x51FA;&#x7684;&#x6587;&#x672C;&#x8F6C;&#x6362;&#x4E3A; UTF-8 &#x5B57;&#x7B26;&#xFF0C;&#x89E3;&#x51B3;&#x4E86;&#x4ECE;&#x7F51;&#x9875;&#x6293;&#x53D6;&#x6587;&#x672C;&#x65F6;&#x5019;&#x4F1A;&#x9047;&#x5230;&#x7684;&#x6587;&#x672C;&#x7F16;&#x7801;&#x7BA1;&#x7406;&#x7684;&#x95EE;&#x9898;&#x3002;</p>
<blockquote>
<p>We&#x2019;ll use the <code>urlopen</code> function from the <code>urllib.request</code> utility to open the web page. Then, we&#x2019;ll use the <code>read</code> function to read the scraped data object. For parsing the data, we&#x2019;ll call the <code>BeautifulSoup</code> object and pass two parameters to it; that is, the <code>article_read</code> and the <code>html.parser</code>.</p>
</blockquote>
<p>&#x6211;&#x4EEC;&#x4F7F;&#x7528; <code>urllib.request</code> &#x7A0B;&#x5E8F;&#x4E2D;&#x7684; <code>urlopen</code> &#x51FD;&#x6570;&#x6765;&#x6253;&#x5F00;&#x7F51;&#x9875;&#x3002;&#x63A5;&#x7740;&#xFF0C;&#x6211;&#x4EEC;&#x4F7F;&#x7528; <code>read</code> &#x51FD;&#x6570;&#x6765;&#x8BFB;&#x53D6;&#x6293;&#x53D6;&#x5230;&#x7684;&#x6570;&#x636E;&#x5BF9;&#x8C61;&#x3002; &#x4E3A;&#x4E86;&#x89E3;&#x6790;&#x6570;&#x636E;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x8C03;&#x7528; <code>BeautifulSoup</code> &#x5BF9;&#x8C61;&#x5E76;&#x4F20;&#x5165;&#x4E24;&#x4E2A;&#x53C2;&#x6570;&#xFF1A;&#x5206;&#x522B;&#x662F; <code>article_read</code> &#x548C; <code>html.parser</code> &#x3002;</p>
<blockquote>
<p>The <code>find_all</code> function is used to return all the <code><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">&gt;</span></span></code> elements present in the HTML. Furthermore, using <code>.text</code> enables us to select only the texts found within the <code><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">&gt;</span></span></code> elements.</p>
</blockquote>
<p>&#x4F7F;&#x7528; <code>find_all</code> &#x51FD;&#x6570;&#x6765;&#x8FD4;&#x56DE; HTML &#x4E2D;&#x51FA;&#x73B0;&#x7684;&#x6240;&#x6709; <code><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">&gt;</span></span></code> &#x5143;&#x7D20;&#x3002;&#x6B64;&#x5916;&#xFF0C;&#x4F7F;&#x7528; <code>.text</code> &#x5C31;&#x53EF;&#x4EE5;&#x9009;&#x53D6;&#x5230; <code><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">&gt;</span></span></code> &#x5143;&#x7D20;&#x4E2D;&#x5305;&#x88F9;&#x7684;&#x6587;&#x672C;&#x4FE1;&#x606F;&#x3002;</p>
<h3 id="&#x7B2C;&#x4E8C;&#x6B65;&#xFF1A;&#x5904;&#x7406;&#x6570;&#x636E;">&#x7B2C;&#x4E8C;&#x6B65;&#xFF1A;&#x5904;&#x7406;&#x6570;&#x636E;</h3>
<blockquote>
<p>To ensure the scrapped textual data is as noise-free as possible, we&#x2019;ll perform some basic text cleaning.  To assist us to do the processing, we&#x2019;ll import a list of <strong>stopwords</strong> from the <strong>nltk</strong> library.</p>
</blockquote>
<p>&#x4E3A;&#x4E86;&#x4FDD;&#x8BC1;&#x6293;&#x53D6;&#x5230;&#x7684;&#x6587;&#x672C;&#x6570;&#x636E;&#x5C3D;&#x53EF;&#x80FD;&#x7684;&#x6CA1;&#x6709;&#x566A;&#x58F0;&#x5E72;&#x6270;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x8FDB;&#x884C;&#x4E00;&#x4E9B;&#x57FA;&#x7840;&#x7684;&#x6570;&#x636E;&#x6E05;&#x6D17;&#x3002;&#x4E3A;&#x4E86;&#x5B8C;&#x6210;&#x8FD9;&#x4E2A;&#x6B65;&#x9AA4;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x4ECE; <strong>nltk</strong> &#x5E93;&#x91CC;&#x5F15;&#x5165;&#x4E00;&#x7CFB;&#x5217;&#x7684;<strong>&#x505C;&#x987F;&#x8BCD;</strong>&#x3002;</p>
<blockquote>
<p>We&#x2019;ll also import <strong>PorterStemmer</strong>, which is an algorithm for reducing words into their root forms. For example, cleaning, cleaned, and cleaner can be reduced to the root clean.</p>
</blockquote>
<p>&#x6211;&#x4EEC;&#x540C;&#x6837;&#x4E5F;&#x4F1A;&#x5F15;&#x5165; <strong>PorterStemmer</strong>,&#x8FD9;&#x662F;&#x4E00;&#x79CD;&#x53EF;&#x4EE5;&#x5C06;&#x8BCD;&#x8BED;&#x8FD8;&#x539F;&#x6210;&#x8BCD;&#x6839;&#x6A21;&#x5F0F;&#x7684;&#x7B97;&#x6CD5;&#x3002;&#x4E3E;&#x4E2A;&#x6817;&#x5B50;&#xFF0C; cleaning, cleaned &#x548C; cleaner &#x5C06;&#x4F1A;&#x88AB;&#x8FD8;&#x539F;&#x6210;&#x6700;&#x6839;&#x672C;&#x7684; clean&#x3002;</p>
<blockquote>
<p>Furthermore, we&#x2019;ll create a dictionary table having the frequency of occurrence of each of the words in the text. We&#x2019;ll loop through the text and the corresponding words to eliminate any stop words.</p>
</blockquote>
<p>&#x6B64;&#x5916;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x65B0;&#x5EFA;&#x4E00;&#x4E2A;&#x8BCD;&#x5178;&#x8868;&#x6765;&#x7EDF;&#x8BA1;&#x6587;&#x672C;&#x4FE1;&#x606F;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5355;&#x8BCD;&#x7684;&#x51FA;&#x73B0;&#x9891;&#x7387;&#x3002;&#x6211;&#x4EEC;&#x5C06;&#x5FAA;&#x73AF;&#x904D;&#x5386;&#x6574;&#x4E2A;&#x6587;&#x672C;&#x548C;&#x76F8;&#x5E94;&#x7684;&#x8BCD;&#x8BED;&#xFF0C;&#x6765;&#x6392;&#x9664;&#x90A3;&#x4E9B;&#x505C;&#x987F;&#x8BCD;&#x3002;</p>
<blockquote>
<p>Then, we&#x2019;ll check if the words are present in the frequency_table. If the word was previously available in the dictionary, its value is updated by 1. Otherwise, if the word is recognized for the first time, its value is set to 1.</p>
</blockquote>
<p>&#x63A5;&#x7740;&#xFF0C;&#x6211;&#x4EEC;&#x8981;&#x786E;&#x8BA4;&#x5355;&#x8BCD;&#x662F;&#x5426;&#x5DF2;&#x7ECF;&#x88AB;&#x8BB0;&#x5F55;&#x8FDB;&#x9891;&#x7387;&#x8868;&#x3002;&#x5982;&#x679C;&#x5355;&#x8BCD;&#x4E4B;&#x524D;&#x5C31;&#x5DF2;&#x7ECF;&#x5728;&#x8BCD;&#x5178;&#x4E2D;&#x5B58;&#x5728;&#xFF0C;&#x5B83;&#x7684;&#x503C;&#x5C06;&#x589E;&#x52A0; 1&#x3002;&#x5426;&#x5219;&#x7684;&#x8BDD;&#xFF0C;&#x5047;&#x5982;&#x8FD9;&#x4E2A;&#x5355;&#x8BCD;&#x662F;&#x7B2C;&#x4E00;&#x6B21;&#x88AB;&#x9A8C;&#x8BC1;&#x7684;&#x8BDD;&#xFF0C;&#x5B83;&#x7684;&#x503C;&#x5C06;&#x4F1A;&#x88AB;&#x8BBE;&#x7F6E;&#x4E3A;1&#x3002;</p>
<blockquote>
<p>For example, the frequency table should look like the following:</p>
</blockquote>
<p>&#x4E3E;&#x4E2A;&#x6817;&#x5B50;&#xFF0C;&#x9891;&#x6570;&#x7EDF;&#x8BA1;&#x8868;&#x5E94;&#x8BE5;&#x770B;&#x8D77;&#x6765;&#x50CF;&#x4E0B;&#x9762;&#x8FD9;&#x4E2A;&#x8868;&#x683C;&#xFF1A;</p>
<div style="text-align:center;">

&#x8BCD;&#x8BED; | &#x9891;&#x6570;
---|---
century | 7
world | 4
United States | 3
computer | 1

</div>

<blockquote>
<p>Here is the code: </p>
</blockquote>
<p>&#x4E0B;&#x9762;&#x662F;&#x4EE3;&#x7801;&#xFF1A;</p>
<pre class="language-"><code class="lang-python"><span class="token keyword">from</span> nltk<span class="token punctuation">.</span>corpus <span class="token keyword">import</span> stopwords
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>stem <span class="token keyword">import</span> PorterStemmer
<span class="token keyword">def</span> <span class="token function">_create_dictionary_table</span><span class="token punctuation">(</span>text_string<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">dict</span><span class="token punctuation">:</span>

    <span class="token comment"># Removing stop words</span>
    stop_words <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>stopwords<span class="token punctuation">.</span>words<span class="token punctuation">(</span><span class="token string">&quot;english&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    words <span class="token operator">=</span> word_tokenize<span class="token punctuation">(</span>text_string<span class="token punctuation">)</span>

    <span class="token comment"># Reducing words to their root form</span>
    stem <span class="token operator">=</span> PorterStemmer<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># Creating dictionary for the word frequency table</span>
    frequency_table <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> wd <span class="token keyword">in</span> words<span class="token punctuation">:</span>
        wd <span class="token operator">=</span> stem<span class="token punctuation">.</span>stem<span class="token punctuation">(</span>wd<span class="token punctuation">)</span>
        <span class="token keyword">if</span> wd <span class="token keyword">in</span> stop_words<span class="token punctuation">:</span>
            <span class="token keyword">continue</span>
        <span class="token keyword">if</span> wd <span class="token keyword">in</span> frequency_table<span class="token punctuation">:</span>
            frequency_table<span class="token punctuation">[</span>wd<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            frequency_table<span class="token punctuation">[</span>wd<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>

    <span class="token keyword">return</span> frequency_table
</code></pre>
<h3 id="&#x7B2C;&#x4E09;&#x6B65;&#xFF1A;&#x5C06;&#x6587;&#x7AE0;&#x62C6;&#x5206;&#x6210;&#x53E5;&#x5B50;">&#x7B2C;&#x4E09;&#x6B65;&#xFF1A;&#x5C06;&#x6587;&#x7AE0;&#x62C6;&#x5206;&#x6210;&#x53E5;&#x5B50;</h3>
<blockquote>
<p>To split the <code>article_content</code> into a set of sentences, we&#x2019;ll use the built-in method from the <strong>nltk</strong> library.</p>
</blockquote>
<p>&#x4E3A;&#x4E86;&#x5C06;&#x6587;&#x7AE0;&#x5185;&#x5BB9;&#x5206;&#x5272;&#x6210;&#x4E00;&#x7CFB;&#x5217;&#x7684;&#x53E5;&#x5B50;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x4F7F;&#x7528; <strong>nltk</strong> &#x5E93;&#x91CC;&#x7684; built-in &#x65B9;&#x6CD5;&#x3002;</p>
<pre class="language-"><code class="lang-python"><span class="token keyword">from</span> nltk<span class="token punctuation">.</span>tokenize <span class="token keyword">import</span> word_tokenize<span class="token punctuation">,</span> sent_tokenize

sentences <span class="token operator">=</span> sent_tokenize<span class="token punctuation">(</span>article<span class="token punctuation">)</span>
</code></pre>
<h3 id="&#x7B2C;&#x56DB;&#x6B65;&#xFF1A;&#x6C42;&#x53E5;&#x5B50;&#x7684;&#x52A0;&#x6743;&#x9891;&#x7387;">&#x7B2C;&#x56DB;&#x6B65;&#xFF1A;&#x6C42;&#x53E5;&#x5B50;&#x7684;&#x52A0;&#x6743;&#x9891;&#x7387;</h3>
<blockquote>
<p>To evaluate the score for every sentence in the text, we&#x2019;ll be analyzing the frequency of occurrence of each term. In this case, we&#x2019;ll be scoring each sentence by its words; that is, adding the frequency of each important word found in the sentence.</p>
</blockquote>
<p>&#x4E3A;&#x4E86;&#x8BC4;&#x4F30;&#x6587;&#x672C;&#x4E2D;&#x6BCF;&#x4E2A;&#x53E5;&#x5B50;&#x7684;&#x5206;&#x6570;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x5206;&#x6790;&#x6BCF;&#x4E00;&#x9879;&#x51FA;&#x73B0;&#x7684;&#x9891;&#x7387;&#x3002;&#x5728;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x6839;&#x636E;&#x6BCF;&#x4E2A;&#x53E5;&#x5B50;&#x4E2D;&#x7684;&#x8BCD;&#x6C47;&#x6765;&#x8FDB;&#x884C;&#x8BC4;&#x5206;&#xFF0C;&#x5C31;&#x662F;&#x8BB2;&#x6BCF;&#x4E2A;&#x53E5;&#x5B50;&#x4E2D;&#x7684;&#x91CD;&#x8981;&#x5355;&#x8BCD;&#x7684;&#x9891;&#x7387;&#x8FDB;&#x884C;&#x52A0;&#x548C;&#x3002;</p>
<blockquote>
<p>Take a look at the following code:</p>
</blockquote>
<p>&#x770B;&#x4E00;&#x4E0B;&#x4E0B;&#x65B9;&#x7684;&#x4EE3;&#x7801;&#xFF1A;</p>
<pre class="language-"><code class="lang-python"><span class="token keyword">def</span> <span class="token function">_calculate_sentence_scores</span><span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> frequency_table<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">dict</span><span class="token punctuation">:</span>   

    <span class="token comment"># Algorithm for scoring a sentence by its words</span>
    sentence_weight <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> sentence <span class="token keyword">in</span> sentences<span class="token punctuation">:</span>
        sentence_wordcount <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>word_tokenize<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        sentence_wordcount_without_stop_words <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">for</span> word_weight <span class="token keyword">in</span> frequency_table<span class="token punctuation">:</span>
            <span class="token keyword">if</span> word_weight <span class="token keyword">in</span> sentence<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                sentence_wordcount_without_stop_words <span class="token operator">+=</span> <span class="token number">1</span>
                <span class="token keyword">if</span> sentence<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span> <span class="token keyword">in</span> sentence_weight<span class="token punctuation">:</span>
                    sentence_weight<span class="token punctuation">[</span>sentence<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">+=</span> frequency_table<span class="token punctuation">[</span>word_weight<span class="token punctuation">]</span>
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    sentence_weight<span class="token punctuation">[</span>sentence<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> frequency_table<span class="token punctuation">[</span>word_weight<span class="token punctuation">]</span>

        sentence_weight<span class="token punctuation">[</span>sentence<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> sentence_weight<span class="token punctuation">[</span>sentence<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">/</span>        sentence_wordcount_without_stop_words

    <span class="token keyword">return</span> sentence_weight
</code></pre>
<blockquote>
<p>Importantly, to ensure long sentences do not have unnecessarily high scores over short sentences, we divided each score of a sentence by the number of words found in that sentence.</p>
</blockquote>
<p>&#x91CD;&#x8981;&#x7684;&#x662F;&#xFF0C;&#x4E3A;&#x4E86;&#x4FDD;&#x8BC1;&#x957F;&#x53E5;&#x4E00;&#x5B9A;&#x4E0D;&#x4F1A;&#x6BD4;&#x77ED;&#x53E5;&#x83B7;&#x5F97;&#x66F4;&#x9AD8;&#x7684;&#x5206;&#x6570;&#xFF0C;&#x6211;&#x4EEC;&#x4F1A;&#x5C06;&#x6BCF;&#x4E2A;&#x53E5;&#x5B50;&#x7684;&#x5F97;&#x5206;&#x9664;&#x4EE5;&#x53E5;&#x5B50;&#x4E2D;&#x53EF;&#x4EE5;&#x627E;&#x5230;&#x7684;&#x5355;&#x8BCD;&#x6570;&#x91CF;&#x3002;</p>
<blockquote>
<p>Also, to optimize the dictionary&#x2019;s memory, we arbitrarily added entence[:7], which refers to the first 7 characters in each sentence. However, for longer documents, where you are likely to encounter sentences with the same first <code>n_chars</code>, it&#x2019;s better to use hash functions or smart index functions to take into account such edge-cases and avoid collisions.</p>
</blockquote>
<p>&#x540C;&#x65F6;&#xFF0C;&#x4E3A;&#x4E86;&#x4F18;&#x5316;&#x8BCD;&#x5178;&#x7684;&#x5B58;&#x50A8;&#xFF0C;&#x6211;&#x4EEC;&#x5C06;&#x968F;&#x673A;&#x7684;&#x52A0;&#x5165;&#x53E5;&#x5B50;[:7]&#xFF0C;&#x5C31;&#x662F;&#x6BCF;&#x4E2A;&#x53E5;&#x5B50;&#x7684;&#x6700;&#x5F00;&#x59CB;&#x7684; 7 &#x4E2A;&#x5B57;&#x7B26;&#x3002;&#x7136;&#x800C;&#xFF0C;&#x5BF9;&#x4E8E;&#x66F4;&#x957F;&#x7684;&#x6587;&#x672C;&#xFF0C;&#x4F60;&#x53EF;&#x80FD;&#x4F1A;&#x9047;&#x5230;&#x5177;&#x6709;&#x76F8;&#x540C;&#x7684;&#x9996;&#x5B57;&#x6BCD; <code>n_chars</code> &#x7684;&#x53E5;&#x5B50;&#x4EEC;&#xFF0C;&#x6240;&#x4EE5;&#x6700;&#x597D;&#x4F7F;&#x7528; hash &#x51FD;&#x6570;&#x6216;&#x667A;&#x80FD;&#x7D22;&#x5F15;&#x51FD;&#x6570;&#x6765;&#x5E94;&#x5BF9;&#x8FD9;&#x79CD;&#x8FB9;&#x7F18;&#x60C5;&#x51B5;&#xFF0C;&#x5E76;&#x907F;&#x514D;&#x51B2;&#x7A81;&#x3002;</p>
<h3 id="&#x7B2C;&#x4E94;&#x6B65;&#xFF1A;&#x8BA1;&#x7B97;&#x53E5;&#x5B50;&#x7684;&#x9608;&#x503C;">&#x7B2C;&#x4E94;&#x6B65;&#xFF1A;&#x8BA1;&#x7B97;&#x53E5;&#x5B50;&#x7684;&#x9608;&#x503C;</h3>
<blockquote>
<p>To further tweak the kind of sentences eligible for summarization, we&#x2019;ll create the average score for the sentences. With this threshold, we can avoid selecting the sentences with a lower score than the average score.</p>
</blockquote>
<p>&#x4E3A;&#x4E86;&#x8FDB;&#x4E00;&#x6B65;&#x8C03;&#x6574;&#x9002;&#x5408;&#x6982;&#x62EC;&#x7684;&#x53E5;&#x5B50;&#x7C7B;&#x578B;&#xFF0C;&#x6211;&#x4EEC;&#x8981;&#x5F97;&#x51FA;&#x8FD9;&#x4E9B;&#x53E5;&#x5B50;&#x7684;&#x5E73;&#x5747;&#x5206;&#x3002;&#x6839;&#x636E;&#x8FD9;&#x4E2A;&#x9608;&#x503C;&#xFF0C;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x907F;&#x514D;&#x9009;&#x5230;&#x5F97;&#x5206;&#x6BD4;&#x5E73;&#x5747;&#x5206;&#x8FD8;&#x4F4E;&#x7684;&#x53E5;&#x5B50;&#x3002;</p>
<blockquote>
<p>Here is the code:</p>
</blockquote>
<p>&#x4E0B;&#x65B9;&#x662F;&#x4EE3;&#x7801;&#xFF1A;</p>
<pre class="language-"><code class="lang-python"><span class="token keyword">def</span> <span class="token function">_calculate_average_score</span><span class="token punctuation">(</span>sentence_weight<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">int</span><span class="token punctuation">:</span>

    <span class="token comment"># Calculating the average score for the sentences</span>
    sum_values <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> entry <span class="token keyword">in</span> sentence_weight<span class="token punctuation">:</span>
        sum_values <span class="token operator">+=</span> sentence_weight<span class="token punctuation">[</span>entry<span class="token punctuation">]</span>

    <span class="token comment"># Getting sentence average value from source text</span>
    average_score <span class="token operator">=</span> <span class="token punctuation">(</span>sum_values <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sentence_weight<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> average_score
</code></pre>
<h3 id="&#x7B2C;&#x516D;&#x6B65;&#xFF1A;&#x83B7;&#x5F97;&#x6458;&#x8981;">&#x7B2C;&#x516D;&#x6B65;&#xFF1A;&#x83B7;&#x5F97;&#x6458;&#x8981;</h3>
<blockquote>
<p>Lastly, since we have all the required parameters, we can now generate a summary for the article.</p>
</blockquote>
<p>&#x6700;&#x540E;&#xFF0C;&#x5F53;&#x6211;&#x4EEC;&#x83B7;&#x5F97;&#x4E86;&#x6240;&#x6709;&#x5FC5;&#x987B;&#x7684;&#x53C2;&#x6570;&#x540E;&#xFF0C;&#x6211;&#x4EEC;&#x73B0;&#x5728;&#x53EF;&#x4EE5;&#x751F;&#x6210;&#x6587;&#x7AE0;&#x7684;&#x6458;&#x8981;&#x5566;&#x3002;</p>
<blockquote>
<p>Here is the code:</p>
</blockquote>
<p>&#x4E0B;&#x9762;&#x662F;&#x4EE3;&#x7801;&#xFF1A;</p>
<pre class="language-"><code class="lang-python"><span class="token keyword">def</span> <span class="token function">_get_article_summary</span><span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> sentence_weight<span class="token punctuation">,</span> threshold<span class="token punctuation">)</span><span class="token punctuation">:</span>
    sentence_counter <span class="token operator">=</span> <span class="token number">0</span>
    article_summary <span class="token operator">=</span> <span class="token string">&apos;&apos;</span>

    <span class="token keyword">for</span> sentence <span class="token keyword">in</span> sentences<span class="token punctuation">:</span>
        <span class="token keyword">if</span> sentence<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span> <span class="token keyword">in</span> sentence_weight <span class="token keyword">and</span> sentence_weight<span class="token punctuation">[</span>sentence<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">&gt;=</span> <span class="token punctuation">(</span>threshold<span class="token punctuation">)</span><span class="token punctuation">:</span>
            article_summary <span class="token operator">+=</span> <span class="token string">&quot; &quot;</span> <span class="token operator">+</span> sentence
            sentence_counter <span class="token operator">+=</span> <span class="token number">1</span>

    <span class="token keyword">return</span> article_summary
</code></pre>
<h2 id="&#x5C3E;&#x58F0;">&#x5C3E;&#x58F0;</h2>
<blockquote>
<p>Here is the entire code for the simple extractive text summarizer in machine learning:</p>
</blockquote>
<p>&#x4E0B;&#x9762;&#x662F;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x4E2D;&#x7B80;&#x5355;&#x63D0;&#x53D6;&#x6587;&#x672C;&#x6458;&#x8981;&#x5668;&#x7684;&#x4EE3;&#x7801;&#xFF1A;</p>
<pre class="language-"><code class="lang-python"><span class="token comment">#importing libraries</span>
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>corpus <span class="token keyword">import</span> stopwords
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>stem <span class="token keyword">import</span> PorterStemmer
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>tokenize <span class="token keyword">import</span> word_tokenize<span class="token punctuation">,</span> sent_tokenize
<span class="token keyword">import</span> bs4 <span class="token keyword">as</span> BeautifulSoup
<span class="token keyword">import</span> urllib<span class="token punctuation">.</span>request  

<span class="token comment">#fetching the content from the URL</span>
fetched_data <span class="token operator">=</span> urllib<span class="token punctuation">.</span>request<span class="token punctuation">.</span>urlopen<span class="token punctuation">(</span><span class="token string">&apos;https://en.wikipedia.org/wiki/20th_century&apos;</span><span class="token punctuation">)</span>

article_read <span class="token operator">=</span> fetched_data<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment">#parsing the URL content and storing in a variable</span>
article_parsed <span class="token operator">=</span> BeautifulSoup<span class="token punctuation">.</span>BeautifulSoup<span class="token punctuation">(</span>article_read<span class="token punctuation">,</span><span class="token string">&apos;html.parser&apos;</span><span class="token punctuation">)</span>

<span class="token comment">#returning &lt;p&gt; tags</span>
paragraphs <span class="token operator">=</span> article_parsed<span class="token punctuation">.</span>find_all<span class="token punctuation">(</span><span class="token string">&apos;p&apos;</span><span class="token punctuation">)</span>

article_content <span class="token operator">=</span> <span class="token string">&apos;&apos;</span>

<span class="token comment">#looping through the paragraphs and adding them to the variable</span>
<span class="token keyword">for</span> p <span class="token keyword">in</span> paragraphs<span class="token punctuation">:</span>  
    article_content <span class="token operator">+=</span> p<span class="token punctuation">.</span>text


<span class="token keyword">def</span> <span class="token function">_create_dictionary_table</span><span class="token punctuation">(</span>text_string<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">dict</span><span class="token punctuation">:</span>

    <span class="token comment">#removing stop words</span>
    stop_words <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>stopwords<span class="token punctuation">.</span>words<span class="token punctuation">(</span><span class="token string">&quot;english&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    words <span class="token operator">=</span> word_tokenize<span class="token punctuation">(</span>text_string<span class="token punctuation">)</span>

    <span class="token comment">#reducing words to their root form</span>
    stem <span class="token operator">=</span> PorterStemmer<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment">#creating dictionary for the word frequency table</span>
    frequency_table <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> wd <span class="token keyword">in</span> words<span class="token punctuation">:</span>
        wd <span class="token operator">=</span> stem<span class="token punctuation">.</span>stem<span class="token punctuation">(</span>wd<span class="token punctuation">)</span>
        <span class="token keyword">if</span> wd <span class="token keyword">in</span> stop_words<span class="token punctuation">:</span>
            <span class="token keyword">continue</span>
        <span class="token keyword">if</span> wd <span class="token keyword">in</span> frequency_table<span class="token punctuation">:</span>
            frequency_table<span class="token punctuation">[</span>wd<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            frequency_table<span class="token punctuation">[</span>wd<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>

    <span class="token keyword">return</span> frequency_table


<span class="token keyword">def</span> <span class="token function">_calculate_sentence_scores</span><span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> frequency_table<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">dict</span><span class="token punctuation">:</span>   

    <span class="token comment">#algorithm for scoring a sentence by its words</span>
    sentence_weight <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> sentence <span class="token keyword">in</span> sentences<span class="token punctuation">:</span>
        sentence_wordcount <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>word_tokenize<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        sentence_wordcount_without_stop_words <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">for</span> word_weight <span class="token keyword">in</span> frequency_table<span class="token punctuation">:</span>
            <span class="token keyword">if</span> word_weight <span class="token keyword">in</span> sentence<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                sentence_wordcount_without_stop_words <span class="token operator">+=</span> <span class="token number">1</span>
                <span class="token keyword">if</span> sentence<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span> <span class="token keyword">in</span> sentence_weight<span class="token punctuation">:</span>
                    sentence_weight<span class="token punctuation">[</span>sentence<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">+=</span> frequency_table<span class="token punctuation">[</span>word_weight<span class="token punctuation">]</span>
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    sentence_weight<span class="token punctuation">[</span>sentence<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> frequency_table<span class="token punctuation">[</span>word_weight<span class="token punctuation">]</span>

        sentence_weight<span class="token punctuation">[</span>sentence<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> sentence_weight<span class="token punctuation">[</span>sentence<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">/</span> sentence_wordcount_without_stop_words



    <span class="token keyword">return</span> sentence_weight

<span class="token keyword">def</span> <span class="token function">_calculate_average_score</span><span class="token punctuation">(</span>sentence_weight<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">int</span><span class="token punctuation">:</span>

    <span class="token comment">#calculating the average score for the sentences</span>
    sum_values <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> entry <span class="token keyword">in</span> sentence_weight<span class="token punctuation">:</span>
        sum_values <span class="token operator">+=</span> sentence_weight<span class="token punctuation">[</span>entry<span class="token punctuation">]</span>

    <span class="token comment">#getting sentence average value from source text</span>
    average_score <span class="token operator">=</span> <span class="token punctuation">(</span>sum_values <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sentence_weight<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> average_score

<span class="token keyword">def</span> <span class="token function">_get_article_summary</span><span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> sentence_weight<span class="token punctuation">,</span> threshold<span class="token punctuation">)</span><span class="token punctuation">:</span>
    sentence_counter <span class="token operator">=</span> <span class="token number">0</span>
    article_summary <span class="token operator">=</span> <span class="token string">&apos;&apos;</span>

    <span class="token keyword">for</span> sentence <span class="token keyword">in</span> sentences<span class="token punctuation">:</span>
        <span class="token keyword">if</span> sentence<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span> <span class="token keyword">in</span> sentence_weight <span class="token keyword">and</span> sentence_weight<span class="token punctuation">[</span>sentence<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">&gt;=</span> <span class="token punctuation">(</span>threshold<span class="token punctuation">)</span><span class="token punctuation">:</span>
            article_summary <span class="token operator">+=</span> <span class="token string">&quot; &quot;</span> <span class="token operator">+</span> sentence
            sentence_counter <span class="token operator">+=</span> <span class="token number">1</span>

    <span class="token keyword">return</span> article_summary

<span class="token keyword">def</span> <span class="token function">_run_article_summary</span><span class="token punctuation">(</span>article<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token comment">#creating a dictionary for the word frequency table</span>
    frequency_table <span class="token operator">=</span> _create_dictionary_table<span class="token punctuation">(</span>article<span class="token punctuation">)</span>

    <span class="token comment">#tokenizing the sentences</span>
    sentences <span class="token operator">=</span> sent_tokenize<span class="token punctuation">(</span>article<span class="token punctuation">)</span>

    <span class="token comment">#algorithm for scoring a sentence by its words</span>
    sentence_scores <span class="token operator">=</span> _calculate_sentence_scores<span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> frequency_table<span class="token punctuation">)</span>

    <span class="token comment">#getting the threshold</span>
    threshold <span class="token operator">=</span> _calculate_average_score<span class="token punctuation">(</span>sentence_scores<span class="token punctuation">)</span>

    <span class="token comment">#producing the summary</span>
    article_summary <span class="token operator">=</span> _get_article_summary<span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> sentence_scores<span class="token punctuation">,</span> <span class="token number">1.5</span> <span class="token operator">*</span> threshold<span class="token punctuation">)</span>

    <span class="token keyword">return</span> article_summary

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">&apos;__main__&apos;</span><span class="token punctuation">:</span>
    summary_results <span class="token operator">=</span> _run_article_summary<span class="token punctuation">(</span>article_content<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>summary_results<span class="token punctuation">)</span>
</code></pre>
<blockquote>
<p>In this case, we applied a threshold of 1.5x of the average score. It&#x2019;s the <a href="https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/" target="_blank">hyperparameter</a> value that generated for us good results after a couple of trials. Of course, you can fine-tune the value according to your preferences and improve the summarization outcomes.</p>
</blockquote>
<p>&#x5728;&#x8FD9;&#x4E2A;&#x6817;&#x5B50;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x4F7F;&#x7528;&#x7684;&#x9608;&#x503C;&#x662F;&#x5E73;&#x5747;&#x5206;&#x7684; 1.5 &#x500D;&#x3002;&#x8FD9;&#x662F;&#x6211;&#x4EEC;&#x901A;&#x8FC7;&#x51E0;&#x6B21;&#x5C1D;&#x8BD5;&#x4E4B;&#x540E;&#x83B7;&#x5F97;&#x7684;&#x597D;&#x7684;&#x7ED3;&#x679C;&#x503C;&#x3002;&#x5F53;&#x7136;&#xFF0C;&#x4F60;&#x53EF;&#x4EE5;&#x6839;&#x636E;&#x4F60;&#x7684;&#x503E;&#x5411;&#x53BB;&#x8C03;&#x6574;&#x8FD9;&#x4E2A;&#x503C;&#x5E76;&#x63D0;&#x5347;&#x6458;&#x8981;&#x5668;&#x7684;&#x8F93;&#x51FA;&#x3002;</p>
<blockquote>
<p>As you can see, running the code summarizes the lengthy Wikipedia article and gives a simplistic overview of the main happenings in the 20th century.</p>
</blockquote>
<p>&#x6B63;&#x5982;&#x4F60;&#x6240;&#x770B;&#x5230;&#x7684;&#x90A3;&#x6837;&#xFF0C;&#x8FD0;&#x884C;&#x4EE3;&#x7801;&#x6765;&#x5BF9;&#x5197;&#x957F;&#x7684;&#x7EF4;&#x57FA;&#x767E;&#x79D1;&#x8FDB;&#x884C;&#x603B;&#x7ED3;&#xFF0C;&#x5E76;&#x63D0;&#x4F9B;&#x4E86;&#x4E00;&#x4E2A;&#x7B80;&#x8981;&#x7684; 20 &#x4E16;&#x7EAA;&#x5927;&#x4E8B;&#x7EAA;&#x7684;&#x56DE;&#x987E;&#x3002;</p>
<blockquote>
<p>Nonetheless, the summary generator can be improved to make it better at producing a concise and precise summary of voluminous texts.</p>
</blockquote>
<p>&#x5C3D;&#x7BA1;&#x5982;&#x6B64;&#xFF0C;&#x6458;&#x8981;&#x751F;&#x6210;&#x5668;&#x53EF;&#x4EE5;&#x4E0D;&#x65AD;&#x7684;&#x63D0;&#x5347;&#x4F18;&#x5316;&#x53BB;&#x4ECE;&#x5197;&#x6742;&#x7684;&#x6587;&#x672C;&#x4E2D;&#x5F97;&#x51FA;&#x66F4;&#x52A0;&#x7CBE;&#x70BC;&#x51C6;&#x786E;&#x7684;&#x603B;&#x7ED3;&#x3002;</p>
<h2 id="&#x63D0;&#x5347;&#x9AD8;&#x5EA6;&#x8BD5;&#x8BD5;">&#x63D0;&#x5347;&#x9AD8;&#x5EA6;&#x8BD5;&#x8BD5;~</h2>
<blockquote>
<p>Of course, this article just brushed the surface of what you can achieve with a text summarization algorithm in machine learning.</p>
</blockquote>
<p>&#x5F53;&#x7136;&#xFF0C;&#x8FD9;&#x7BC7;&#x6587;&#x7AE0;&#x4EC5;&#x4EC5;&#x53EA;&#x662F;&#x7247;&#x9762;&#x7684;&#x4ECB;&#x7ECD;&#x4E86;&#x80FD;&#x901A;&#x8FC7;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x5B8C;&#x6210;&#x7684;&#x4E00;&#x4E2A;&#x6587;&#x672C;&#x6458;&#x8981;&#x7B97;&#x6CD5;&#x3002;</p>
<blockquote>
<p>To learn more about the subject, especially about abstractive text summarization, here are some useful resources you can use:</p>
</blockquote>
<p>&#x4E3A;&#x4E86;&#x62D3;&#x5C55;&#x5B66;&#x4E60;&#x8FD9;&#x4E2A;&#x8BFE;&#x9898;&#xFF0C;&#x7279;&#x522B;&#x662F;&#x5173;&#x4E8E;&#x62BD;&#x8C61;&#x6587;&#x672C;&#x6458;&#x8981;&#xFF0C;&#x4E0B;&#x9762;&#x662F;&#x4E00;&#x4E9B;&#x6709;&#x7528;&#x7684;&#x8D44;&#x6E90;&#x53EF;&#x4F9B;&#x4F7F;&#x7528;&#xFF1A;</p>
<blockquote>
<ul>
<li>Is it possible to combine the two approaches (abstractive and extractive)? It is the main idea behind the pointer-generator network that gets the best of both worlds by combining both extraction(pointing) and abstraction(generating).</li>
</ul>
</blockquote>
<ul>
<li>&#x662F;&#x5426;&#x6709;&#x53EF;&#x80FD;&#x7EC4;&#x5408;&#x4E24;&#x79CD;&#x65B9;&#x6CD5;&#xFF08;&#x62BD;&#x8C61;&#x548C;&#x63D0;&#x53D6;&#xFF09;&#x5462;&#xFF1F;&#x8FD9;&#x4E2A;&#x662F; &#x6307;&#x5411;-&#x751F;&#x6210;&#x5668; &#x7F51;&#x7EDC;&#x7684;&#x6700;&#x4E3B;&#x8981;&#x89C2;&#x70B9;&#xFF1A;&#x8BA9;&#x63D0;&#x53D6;&#xFF08;&#x6307;&#x5411;&#xFF09;&#x548C;&#x62BD;&#x8C61;&#xFF08;&#x751F;&#x6210;&#xFF09;&#x4E24;&#x79CD;&#x65B9;&#x6CD5;&#x7ED3;&#x5408;&#x5F97;&#x5230;&#x66F4;&#x597D;&#x7684;&#x6548;&#x679C;&#x3002;</li>
</ul>
<blockquote>
<ul>
<li><a href="https://arxiv.org/pdf/1810.09305.pdf" target="_blank">How to use WikiHow, a large-scale text summarization dataset</a>&#x2014;This paper introduces WikiHow, a new large-scale text summarization dataset that comprises of more than 230,000 articles extracted from the WikiHow online knowledge base. Most of the presently available datasets are not large enough for training sequence-to-sequence modelsmodels, they may provide only limited summaries, and they are more suited to performing extractive summarization. However, the WikiHow dataset is large-scale, high-quality, and capable of achieving optimal results in abstractive summarization.</li>
</ul>
</blockquote>
<ul>
<li><a href="https://arxiv.org/pdf/1810.09305.pdf" target="_blank">&#x5982;&#x4F55;&#x4F7F;&#x7528; WikiHow&#xFF0C;&#x4E00;&#x4E2A;&#x5DE8;&#x5927;&#x89C4;&#x6A21;&#x7684;&#x6587;&#x672C;&#x6458;&#x8981;&#x6570;&#x636E;&#x96C6;</a>&#x2014;&#x8FD9;&#x7BC7;&#x8BBA;&#x6587;&#x4ECB;&#x7ECD;&#x4E86; WikiHow&#xFF0C;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x62E5;&#x6709;&#x5DE8;&#x5927;&#x89C4;&#x6A21;&#x7684;&#x6587;&#x672C;&#x6458;&#x8981;&#x6570;&#x636E;&#x96C6;&#xFF0C;&#x5B83;&#x7531;&#x8D85;&#x8FC7;&#x4E86; 230,000 &#x7BC7;&#x4ECE; Wiki How &#x5728;&#x7EBF;&#x77E5;&#x8BC6;&#x5E93;&#x63D0;&#x53D6;&#x7684;&#x6587;&#x7AE0;&#x7EC4;&#x6210;&#x3002;&#x73B0;&#x5B58;&#x5DF2;&#x6709;&#x7684;&#x5927;&#x591A;&#x6570;&#x6570;&#x636E;&#x96C6;&#x7684;&#x4F53;&#x91CF;&#x90FD;&#x4E0D;&#x8DB3;&#x4EE5;&#x5B8C;&#x6210; sequence-to-sequence &#x6A21;&#x578B;&#x7684;&#x8BAD;&#x7EC3;&#xFF0C;&#x4ED6;&#x4EEC;&#x6709;&#x7684;&#x53EA;&#x63D0;&#x4F9B;&#x6709;&#x9650;&#x7684;&#x6458;&#x8981;&#xFF0C;&#x8FD9;&#x6837;&#x4F1A;&#x66F4;&#x9002;&#x5408;&#x8FDB;&#x884C;&#x6458;&#x8981;&#x7684;&#x63D0;&#x53D6;&#x3002;&#x7136;&#x800C;&#xFF0C; WikiHow &#x6570;&#x636E;&#x96C6;&#x662F;&#x4E00;&#x4E2A;&#x5927;&#x89C4;&#x6A21;&#xFF0C;&#x9AD8;&#x8D28;&#x91CF;&#xFF0C;&#x800C;&#x4E14;&#x80FD;&#x5728;&#x62BD;&#x8C61;&#x7ED3;&#x679C;&#x4E2D;&#x83B7;&#x5F97;&#x6700;&#x6709;&#x7ED3;&#x679C;&#x3002;</li>
</ul>
<blockquote>
<ul>
<li><a href="https://arxiv.org/pdf/1902.09243.pdf" target="_blank">How a pretraining-based encoder-decoder framework can be used in text summarization</a>&#x2014;This paper introduces a unique two-stage model that is based on a sequence-to-sequence paradigm. The model makes use of <a href="https://arxiv.org/abs/1810.04805" target="_blank">BERT</a> (<a href="https://blog.floydhub.com/ten-trends-in-deep-learning-nlp/" target="_blank">you can bet that we will continue to read about BERT in all 2019</a>) on both encoder and decoder sides and focuses on reinforced objective during the learning process. When the model was assessed on some benchmark datasets, the outcome revealed that the approach performed better at text summarization, particularly when compared to other traditional systems.</li>
</ul>
</blockquote>
<ul>
<li><a href="https://arxiv.org/pdf/1902.09243.pdf" target="_blank">&#x5982;&#x4F55;&#x5728;&#x6587;&#x672C;&#x6458;&#x8981;&#x4E2D;&#x4F7F;&#x7528;&#x9884;&#x8BAD;&#x7EC3;&#x7684;&#x7F16;&#x89E3;&#x7801;&#x6846;&#x67B6;</a>&#x2014;&#x8FD9;&#x7BC7;&#x8BBA;&#x6587;&#x4ECB;&#x7ECD;&#x4E86;&#x4E00;&#x4E2A;&#x72EC;&#x7279;&#x7684;&#x57FA;&#x4E8E; sequence-to-sequence &#x8303;&#x5F0F;&#x7684;&#x4E24;&#x6BB5;&#x5F0F;&#x6A21;&#x578B;&#x3002;&#x8FD9;&#x4E2A;&#x6A21;&#x578B;&#x5728;&#x89E3;&#x7801;&#x548C;&#x7F16;&#x7801;&#x90FD;&#x4F7F;&#x7528;&#x4E86; <a href="https://arxiv.org/abs/1810.04805" target="_blank">BERT</a> (<a href="https://blog.floydhub.com/ten-trends-in-deep-learning-nlp/" target="_blank">&#x4F60;&#x53EF;&#x4EE5;&#x786E;&#x4FE1;&#x6211;&#x4EEC;&#x5C06;&#x5728;&#x6574;&#x4E2A; 2019 &#x7EE7;&#x7EED;&#x7814;&#x7A76; BERT </a>)&#xFF0C;&#x5E76;&#x4E14;&#x5728;&#x5B66;&#x4E60;&#x8FC7;&#x7A0B;&#x4E2D;&#x805A;&#x7126;&#x4E8E;&#x589E;&#x5F3A;&#x76EE;&#x6807;&#x3002;&#x5F53;&#x57FA;&#x4E8E;&#x4E00;&#x4E9B;&#x57FA;&#x51C6;&#x6570;&#x636E;&#x5BF9;&#x6A21;&#x578B;&#x8FDB;&#x884C;&#x8BC4;&#x4F30;&#x65F6;&#xFF0C;&#x7ED3;&#x679C;&#x8868;&#x660E;&#x8BE5;&#x65B9;&#x6CD5;&#x5728;&#x6587;&#x672C;&#x6458;&#x8981;&#x4E0A;&#x8868;&#x73B0;&#x7684;&#x66F4;&#x597D;&#xFF0C;&#x5C24;&#x5176;&#x662F;&#x548C;&#x5176;&#x4ED6;&#x4F20;&#x7EDF;&#x7CFB;&#x7EDF;&#x8FDB;&#x884C;&#x5BF9;&#x6BD4;&#x7684;&#x65F6;&#x5019;&#x3002;</li>
</ul>
<h1 id="&#x751F;&#x8BCD;&#x4E0E;&#x77ED;&#x8BED;">&#x751F;&#x8BCD;&#x4E0E;&#x77ED;&#x8BED;</h1>
<ul>
<li>time consuming</li>
</ul>
<blockquote>
<p>&#x6D6A;&#x8D39;&#x65F6;&#x95F4;&#x7684;</p>
</blockquote>
<ul>
<li>tedious</li>
</ul>
<blockquote>
<p>adj. &#x6C89;&#x95F7;&#x7684;&#xFF1B;&#x5197;&#x957F;&#x4E4F;&#x5473;&#x7684;</p>
</blockquote>
<ul>
<li>redundant</li>
</ul>
<blockquote>
<p>adj. &#x591A;&#x4F59;&#x7684;&#xFF0C;&#x8FC7;&#x5269;&#x7684;&#xFF1B;&#x88AB;&#x89E3;&#x96C7;&#x7684;&#xFF0C;&#x5931;&#x4E1A;&#x7684;&#xFF1B;&#x5197;&#x957F;&#x7684;&#xFF0C;&#x7D2F;&#x8D58;&#x7684;</p>
</blockquote>
<ul>
<li>concise</li>
</ul>
<blockquote>
<p>adj. &#x7B80;&#x660E;&#x7684;&#xFF0C;&#x7B80;&#x6D01;&#x7684;</p>
</blockquote>
<ul>
<li>precise</li>
</ul>
<blockquote>
<p>adj. &#x7CBE;&#x786E;&#x7684;&#xFF1B;&#x660E;&#x786E;&#x7684;&#xFF1B;&#x4E25;&#x683C;&#x7684;</p>
</blockquote>
<ul>
<li>voluminous</li>
</ul>
<blockquote>
<p>adj. &#x5927;&#x91CF;&#x7684;&#xFF1B;&#x591A;&#x5377;&#x7684;&#xFF0C;&#x957F;&#x7BC7;&#x7684;&#xFF1B;&#x8457;&#x4E66;&#x591A;&#x7684;</p>
</blockquote>
<ul>
<li>collapsed</li>
</ul>
<blockquote>
<p>adj. &#x5012;&#x584C;&#x7684;&#xFF1B;&#x66B4;&#x8DCC;&#x7684;&#xFF1B;&#x6536;&#x7F29;&#x7684;&#xFF1B;&#x503E;&#x9677;&#x4E86;&#x7684;</p>
<p>v. &#x5012;&#x584C;&#xFF1B;&#x5D29;&#x6E83;&#xFF08;collapse&#x7684;&#x8FC7;&#x53BB;&#x5206;&#x8BCD;&#xFF09;&#xFF1B;&#x4EF7;&#x683C;&#x66B4;&#x8DCC;</p>
</blockquote>
<ul>
<li>corresponding</li>
</ul>
<blockquote>
<p>adj. &#x76F8;&#x5F53;&#x7684;&#xFF0C;&#x76F8;&#x5E94;&#x7684;&#xFF1B;&#x4E00;&#x81F4;&#x7684;&#xFF1B;&#x901A;&#x4FE1;&#x7684;</p>
<p>v. &#x7C7B;&#x4F3C;&#xFF08;correspond&#x7684;ing&#x5F62;&#x5F0F;&#xFF09;&#xFF1B;&#x76F8;&#x914D;</p>
</blockquote>
<ul>
<li>assists in</li>
</ul>
<blockquote>
<p>&#x6709;&#x52A9;&#x4E8E;</p>
</blockquote>
<ul>
<li>insignificant</li>
</ul>
<blockquote>
<p>adj. &#x65E0;&#x5173;&#x7D27;&#x8981;&#x7684;</p>
</blockquote>
<ul>
<li>representative</li>
</ul>
<blockquote>
<p>n. &#x4EE3;&#x8868;&#xFF1B;&#x5178;&#x578B;&#xFF1B;&#x4F17;&#x8BAE;&#x5458;</p>
<p>adj. &#x5178;&#x578B;&#x7684;&#xFF0C;&#x6709;&#x4EE3;&#x8868;&#x6027;&#x7684;&#xFF1B;&#x4EE3;&#x8BAE;&#x5236;&#x7684;</p>
</blockquote>
<ul>
<li>eliminate</li>
</ul>
<blockquote>
<p>vt. &#x6D88;&#x9664;&#xFF1B;&#x6392;&#x9664;</p>
</blockquote>
<ul>
<li>take into account</li>
</ul>
<blockquote>
<p>&#x8003;&#x8651;&#xFF1B;&#x91CD;&#x89C6;&#xFF1B;&#x4F53;&#x8C05;</p>
</blockquote>
<ul>
<li>tweak</li>
</ul>
<blockquote>
<p>n. &#x626D;&#xFF1B;&#x8F7B;&#x5FAE;&#x8C03;&#x6574;</p>
<p>v. &#x626D;&#xFF1B;&#xFF08;&#x975E;&#x6B63;&#x5F0F;&#xFF09;&#x7A0D;&#x5FAE;&#x8C03;&#x6574;&#x673A;&#x5668;&#x6216;&#x7CFB;&#x7EDF;&#xFF1B;&#x7528;&#x529B;&#x62C9;&#xFF1B;&#x7126;&#x6025;</p>
</blockquote>
<ul>
<li>sophisticated</li>
</ul>
<blockquote>
<p>adj. &#x590D;&#x6742;&#x7684;&#xFF1B;&#x7CBE;&#x81F4;&#x7684;&#xFF1B;&#x4E45;&#x7ECF;&#x4E16;&#x6545;&#x7684;&#xFF1B;&#x5BCC;&#x6709;&#x7ECF;&#x9A8C;&#x7684;</p>
<p>v. &#x4F7F;&#x53D8;&#x5F97;&#x4E16;&#x6545;&#xFF1B;&#x4F7F;&#x8FF7;&#x60D1;&#xFF1B;&#x7BE1;&#x6539;&#xFF08;sophisticate&#x7684;&#x8FC7;&#x53BB;&#x5206;&#x8BCD;&#x5F62;&#x5F0F;&#xFF09;</p>
</blockquote>
<ul>
<li>semantic representation</li>
</ul>
<blockquote>
<p>&#x8BED;&#x4E49;&#x8868;&#x793A;</p>
</blockquote>
<ul>
<li>inference permutation</li>
</ul>
<blockquote>
<p>&#x63A8;&#x7406;&#x6392;&#x5217; </p>
</blockquote>
<footer class="page-footer"><span class="copyright">JalanJiang.&#x6C5F;&#x5B50;&#x6291; all right reserved&#xFF0C;powered by Gitbook</span><span class="footer-modification">&#x8BE5;&#x6587;&#x4EF6;&#x4FEE;&#x8BA2;&#x65F6;&#x95F4;&#xFF1A;
2019-05-26 21:37:52
</span></footer>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="the-downsides-of-json-for-config-files.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page: 用 JSON 作为配置文件的缺陷">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"浅谈机器学习中的文本摘要","level":"4.3","depth":1,"previous":{"title":"用 JSON 作为配置文件的缺陷","level":"4.2","depth":1,"path":"other/the-downsides-of-json-for-config-files.md","ref":"other/the-downsides-of-json-for-config-files.md","articles":[]},"dir":"ltr"},"config":{"plugins":["theme-default","disqus","prism","-highlight","tbfed-pagefooter","github-buttons","page-toc-button","back-to-top-button","sitemap-general","favicon@^0.0.2","splitter","baiduana"],"styles":{"ebook":"styles/ebook.css","epub":"styles/epub.css","mobi":"styles/mobi.css","pdf":"styles/pdf.css","print":"styles/print.css","website":"styles/website.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"JalanJiang.江子抑","modify_label":"该文件修订时间：","modify_format":"YYYY-MM-DD HH:mm:ss"},"prism":{"css":["prismjs/themes/prism-twilight.css"]},"disqus":{"useIdentifier":false,"shortName":"jalanspace"},"splitter":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sitemap-general":{"prefix":"http://jalan.space/leetcode-notebook/"},"fontsettings":{"theme":"white","family":"sans","size":2},"fontSettings":{"theme":"sepia","family":"serif","size":2},"favicon":{"shortcut":"favicon.ico","bookmark":"favicon.ico"},"page-toc-button":{},"back-to-top-button":{},"github-buttons":{"buttons":[{"user":"JalanJiang","repo":"weekly-translation","type":"star","size":"small"}]},"baiduana":{},"baidu":{"token":"c47c7dbbbbb8b1bd7729048f358cd896"},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"theme-default":{"showLevel":true,"styles":{"ebook":"styles/ebook.css","epub":"styles/epub.css","mobi":"styles/mobi.css","pdf":"styles/pdf.css","print":"styles/print.css","website":"styles/website.css"}}},"theme":"default","author":"JalanJiang.江子抑 <jjy@meitu.com>","pdf":{"pageBreaksBefore":"/","headerTemplate":null,"paperSize":"a4","margin":{"right":62,"left":62,"top":36,"bottom":36},"fontSize":12,"fontFamily":"Arial","footerTemplate":null,"chapterMark":"pagebreak","pageNumbers":false},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"isbn":"","variables":{},"title":"每周翻译计划","links":{"sidebar":{"我的博客":"http://jalan.space"},"sharing":{"google":null,"facebook":null,"twitter":null,"weibo":null,"all":null}},"gitbook":"*","description":"每周翻译一篇英文技术文章~","extension":""},"file":{"path":"other/a-gentle-introduction-to-text-summarization-in-machine-learning.md","mtime":"2019-05-26T13:37:52.910Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2019-05-26T13:47:04.858Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/URI.js/1.16.1/URI.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-disqus/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-page-toc-button/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-baiduana/baidu_gitbook.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

